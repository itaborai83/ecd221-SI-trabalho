{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itaborai83/ecd221-SI-trabalho/blob/main/Trabalho_ECD221_ENGENHARIA_DE_SOFTWARE_PARA_CI%C3%8ANCIA_DE_DADOS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnubmiitCxpa"
      },
      "source": [
        "# ECD221 - ENGENHARIA DE SOFTWARE PARA CIÊNCIA DE DADOS\n",
        "\n",
        "**Integrantes**\n",
        "\n",
        "- Rafaela Schifino\n",
        "- Renan Giordano Sfirri\n",
        "- Agnello Hupp\n",
        "- Daniel Lemos Itaboraí\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swFAc1KX3BnV"
      },
      "source": [
        "# User Story\n",
        "**COMO** gerente de relacionamento **QUERO** predizer o cancelamento de um serviço de acordo com um conjunto de características **PARA** prevenir o cancelamento (churn rate) e melhorar a rentabilidade da empresa.\n",
        "\n",
        "**Critérios de Aceitação:**\n",
        "DADO que existe um conjunto de informações e características de um cliente\n",
        "QUANDO executo o modelo de Machine Learning\n",
        "ENTÃO obtenho uma indicação se um serviço será cancelado pelo cliente.\n",
        "\n",
        "**Detalhamento:**\n",
        "#####Objetivo: Predizer se um determinado cliente irá cancelar um serviço considerando seu histórico e um conjunto de características.\n",
        "#####Objetivos organizacionais: Redizir a taxa de cancelamento melhorando a rentabilidade da empresa.\n",
        "#####Objetivo do modelo: Predizer com no mínimo 75% de acurácia, em amostras não utilizadas no treinamento, se um determinado cliente irá cancelar um serviço.\n",
        "\n",
        "\n",
        "\t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHSiKCs9DFso"
      },
      "source": [
        "## Carga dos Arquivos via \\%\\%writefile\n",
        "\n",
        "Código também disponível via https://github.com/itaborai83/ecd221-SI-trabalho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL-k22EN7eL5"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ./telchurn\n",
        "!mkdir -p ./DATA\n",
        "!mkdir -p ./MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWFJUg136xyA",
        "outputId": "8a3d1af4-87ba-4733-bdb0-8e54a0d7885c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/__init__.py\n",
        "# empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpO47Voa75pL",
        "outputId": "4e19e309-08bc-4115-ad0a-55fcb7a746cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/util.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/util.py\n",
        "import io\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import logging\n",
        "import datetime as dt\n",
        "\n",
        "LOGGER_FORMAT = '%(asctime)s:%(levelname)s:%(filename)s:%(funcName)s:%(lineno)d\\n\\t%(message)s\\n'\n",
        "LOGGER_FORMAT = '%(levelname)s - %(filename)s:%(funcName)s:%(lineno)s - %(message)s'\n",
        "stdout_handler = logging.StreamHandler(stream=sys.stdout)\n",
        "logging.basicConfig(level=logging.INFO, format=LOGGER_FORMAT, handlers=[stdout_handler])\n",
        "logging.basicConfig(level=logging.INFO, format=LOGGER_FORMAT)\n",
        "\n",
        "def get_logger(name):\n",
        "    return logging.getLogger(name)\n",
        "\n",
        "def report_df(logger, df):\n",
        "    buffer = io.StringIO()\n",
        "    df.info(verbose=True, buf=buffer)\n",
        "    buffer.seek(0)\n",
        "    logger.info(buffer.read())\n",
        "\n",
        "def silence_warnings():\n",
        "    # to silence warnings of subprocesses\n",
        "    if not sys.warnoptions:\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        os.environ[\"PYTHONWARNINGS\"] = \"ignore::UserWarning,ignore::FutureWarning\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2eGJFaf8QCI",
        "outputId": "e50225c0-a846-435f-9704-fdf189b6ca87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/data_loader.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/data_loader.py\n",
        " # -*- coding: utf-8 -*-\n",
        "import io\n",
        "import sys\n",
        "import abc\n",
        "import pandas as pd\n",
        "import telchurn.util as util\n",
        "\n",
        "LOGGER = util.get_logger('data_loader')\n",
        "\n",
        "class DataLoader(abc.ABC):\n",
        "    \n",
        "    DELIMITER = ','\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def load(self, file_name_or_url: str) -> pd.DataFrame:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    @abc.abstractmethod\n",
        "    def load_cleansed(self, file_name_or_url: str) -> pd.DataFrame:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "        \n",
        "class DataLoaderImpl(DataLoader):\n",
        "    FIELD_SEPARATOR             = \",\"\n",
        "    IMPORT_COLUMN_NAMES         = [\n",
        "        \"customer_id\"\n",
        "    ,   \"gender\"\n",
        "    ,   \"senior_citizen\"\n",
        "    ,   \"partner\"\n",
        "    ,   \"dependents\"\n",
        "    ,   \"tenure\"\n",
        "    ,   \"phone_service\"\n",
        "    ,   \"multiple_lines\"\n",
        "    ,   \"internet_service\"\n",
        "    ,   \"online_security\"\n",
        "    ,   \"online_backup\"\n",
        "    ,   \"device_protection\"\n",
        "    ,   \"tech_support\"\n",
        "    ,   \"streaming_tv\"\n",
        "    ,   \"streaming_movies\"\n",
        "    ,   \"contract\"\n",
        "    ,   \"paperless_billing\"\n",
        "    ,   \"payment_method\"\n",
        "    ,   \"monthly_charges\"\n",
        "    ,   \"total_charges\"\n",
        "    ,   \"churn\"\n",
        "    ]\n",
        "    SKIP_ROWS = 1\n",
        "    \n",
        "    def load(self, file_name_or_url: str) -> pd.DataFrame:\n",
        "        LOGGER.info(f'loading dataframe from {file_name_or_url}')\n",
        "        churn_df = pd.read_csv(\n",
        "            file_name_or_url\n",
        "        ,   names     = self.IMPORT_COLUMN_NAMES\n",
        "        ,   skiprows  = 1\n",
        "        ,   delimiter = self.DELIMITER\n",
        "        )\n",
        "        util.report_df(LOGGER, churn_df)\n",
        "        return churn_df\n",
        "        \n",
        "    def load_cleansed(self, file_name_or_url: str) -> pd.DataFrame:\n",
        "        LOGGER.info(f'loading cleansed dataframe from {file_name_or_url}')\n",
        "        churn_df = pd.read_csv(\n",
        "            file_name_or_url\n",
        "        ,   delimiter = self.DELIMITER\n",
        "        )\n",
        "        util.report_df(LOGGER, churn_df)\n",
        "        return churn_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dycTCXIM8bB0",
        "outputId": "1c236c90-57de-48dd-a968-33e8b3d39af3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/data_splitter.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/data_splitter.py\n",
        " # -*- coding: utf-8 -*-\n",
        "import abc\n",
        "import pandas as pd\n",
        "from typing import Tuple, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler, ADASYN, SMOTE\n",
        "import telchurn.util as util\n",
        "\n",
        "LOGGER = util.get_logger('data_splitter')\n",
        "\n",
        "class DataSplitter(abc.ABC):\n",
        "    \n",
        "    DEFAULT_TEST_PCT_SIZE   = 0.3 # 30% do conjunto de dados\n",
        "    DEFAULT_RANDOM_STATE    = 42\n",
        "    \n",
        "    def __init__(self, seed: int=None, test_split_pct: float=None):\n",
        "        self.seed = seed if seed else self.DEFAULT_RANDOM_STATE\n",
        "        self.test_split_pct = test_split_pct if test_split_pct else self.DEFAULT_TEST_PCT_SIZE        \n",
        "        \n",
        "    @abc.abstractmethod\n",
        "    def split(self, df: pd.DataFrame, target: str) -> Tuple[Tuple[pd.DataFrame, pd.DataFrame], Tuple[pd.DataFrame, pd.DataFrame]]:\n",
        "        raise NotImplementedError\n",
        "            \n",
        "class DataSplitterImpl(DataSplitter):\n",
        "    \n",
        "    def __init__(self, seed: int=None, test_split_pct: float=None):\n",
        "        super().__init__(seed, test_split_pct)\n",
        "        \n",
        "    def _oversample(self, X_train, y_train):\n",
        "        return X_train.copy(), y_train.copy()\n",
        "\n",
        "    def split(self, df: pd.DataFrame, target: str) -> Tuple[Tuple[pd.DataFrame, pd.DataFrame], Tuple[pd.DataFrame, pd.DataFrame]]:\n",
        "        LOGGER.info('splitting data set into train and test sets')\n",
        "        all_but_target = df.columns.difference([target])\n",
        "        X_df = df[all_but_target]\n",
        "        y = df[target]\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_df.values\n",
        "        ,   y\n",
        "        ,   test_size     = self.test_split_pct\n",
        "        ,   shuffle       = True\n",
        "        ,   random_state  = self.seed\n",
        "        ,   stratify      = y # com estratificação\n",
        "        )\n",
        "        X_train_resampled, y_train_resampled = self._oversample(X_train, y_train)\n",
        "        X_train_df = pd.DataFrame(X_train_resampled, columns=X_df.columns)\n",
        "        y_train_df = pd.DataFrame(y_train_resampled, columns=[target])\n",
        "        X_test_df = pd.DataFrame(X_test, columns=X_df.columns)\n",
        "        y_test_df = pd.DataFrame(y_test, columns=[target])\n",
        "        return (X_train_df, y_train_df), (X_test_df, y_test_df)\n",
        "\n",
        "# os data splitters com oversampling abaixo não foram usados pois não \n",
        "# apresentaram ganho significativo de performance e aumentavam o tempo de treino\n",
        "\n",
        "class DataSplitterOverSamplerImpl(DataSplitterImpl):\n",
        "\n",
        "    def __init__(self, seed: int=None, test_split_pct: float=None):\n",
        "        super().__init__(seed, test_split_pct)\n",
        "\n",
        "    def _oversample(self, X_train, y_train):\n",
        "        ros = RandomOverSampler(random_state=self.seed)\n",
        "        return ros.fit_resample(X_train, y_train)\n",
        "        \n",
        "class DataSplitterSmoteImpl(DataSplitterImpl):\n",
        "    \n",
        "    def __init__(self, seed: int=None, test_split_pct: float=None):\n",
        "        super().__init__(seed, test_split_pct)\n",
        "\n",
        "    def _oversample(self, X_train, y_train):\n",
        "        smote = SMOTE(random_state=self.seed)\n",
        "        return smote.fit_resample(X_train, y_train)\n",
        "        \n",
        "class DataSplitterAdasynImpl(DataSplitterImpl):\n",
        "    \n",
        "    def __init__(self, seed: int=None, test_split_pct: float=None):\n",
        "        super().__init__(seed, test_split_pct)\n",
        "\n",
        "    def _oversample(self, X_train, y_train):\n",
        "        adasyn = ADASYN(random_state=self.seed)\n",
        "        return adasyn.fit_resample(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKgrcBNq8nrB",
        "outputId": "57528cd1-04e0-45b1-e87e-6f43016c959b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/feature_processor.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/feature_processor.py\n",
        " # -*- coding: utf-8 -*-\n",
        "import io\n",
        "import abc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import telchurn.util as util\n",
        "\n",
        "LOGGER = util.get_logger('feature_processor')\n",
        "\n",
        "class FeatureProcessor(abc.ABC):\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def handle_categorical_features(self, churn_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def engineer_features(self, churn_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "class FeatureProcessorImpl(FeatureProcessor):\n",
        "    \n",
        "    BOOLEAN_FEATURES = [\n",
        "        \"senior_citizen\"\n",
        "    ,   \"partner\"\n",
        "    ,   \"dependents\"\n",
        "    ,   \"phone_service\"\n",
        "    ,   \"paperless_billing\"\n",
        "    ]    \n",
        "    \n",
        "    CATEGORICAL_FEATURES = [\n",
        "        \"multiple_lines\"\n",
        "    ,   \"internet_service\"\n",
        "    ,   \"online_security\"\n",
        "    ,   \"online_backup\"\n",
        "    ,   \"device_protection\"\n",
        "    ,   \"tech_support\"\n",
        "    ,   \"streaming_tv\"\n",
        "    ,   \"streaming_movies\"\n",
        "    ,   \"contract\"\n",
        "    ,   \"payment_method\"\n",
        "    ]\n",
        "    \n",
        "    NUMERICAL_FEATURES = [\n",
        "        \"tenure\"\n",
        "    ,   \"monthly_charges\"\n",
        "    ,   \"total_charges\"\n",
        "    ]\n",
        "    \n",
        "    TARGET_VARIABLE = \"churn\"\n",
        "    \n",
        "    BOOLEAN_MAP = {\"No\": 0, \"Yes\": 1}\n",
        "    \n",
        "    ENABLE_FEATURE_ENGINEERING = False\n",
        "\n",
        "    # ruído a ser adicionado para evitar overfitting e fazer as variáveis novas parecerem numéricas\n",
        "    NOISE_STD = 0.0 # zerado por falta de tempo para averiguar se ajudava ou não com overfitting\n",
        "    \n",
        "    def __init__(self, seed):\n",
        "        self.seed = seed\n",
        "        \n",
        "    def handle_categorical_features(self, churn_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        LOGGER.info('handling categorical features')\n",
        "        # copia o data frame para não estragar os dados originais\n",
        "        churn_df = churn_df.copy()\n",
        "        \n",
        "        # transformando variáveis booleanas em numéricas (dummy encoding não é necessário)\n",
        "        for feature in self.BOOLEAN_FEATURES:\n",
        "          churn_df[feature] = churn_df[feature].map(self.BOOLEAN_MAP)\n",
        "\n",
        "        # realizando o dummy encoding usando pandas\n",
        "        dummy_df = pd.get_dummies(\n",
        "            data        = churn_df[self.CATEGORICAL_FEATURES]\n",
        "        ,   prefix      = self.CATEGORICAL_FEATURES\n",
        "        ,   prefix_sep  = \"=\"\n",
        "        )\n",
        "\n",
        "        # concatenando as variáveis boleanas, categóricas codificadas, numéricas e variável target num novo dataset\n",
        "        churn_df = pd.concat([\n",
        "            churn_df[ self.BOOLEAN_FEATURES ]\n",
        "        ,   dummy_df\n",
        "        ,   churn_df[ self.NUMERICAL_FEATURES ]\n",
        "        ,   churn_df[ self.TARGET_VARIABLE ]\n",
        "        ], axis=1)\n",
        "\n",
        "        # removendo variáveis equivalentes internet_service=No = 1\n",
        "        del churn_df[ \"device_protection=No internet service\" ]\n",
        "        del churn_df[ \"streaming_tv=No internet service\"      ]\n",
        "        del churn_df[ \"tech_support=No internet service\"      ]\n",
        "        del churn_df[ \"online_backup=No internet service\"     ]\n",
        "        del churn_df[ \"streaming_movies=No internet service\"  ]\n",
        "        del churn_df[ \"online_security=No internet service\"   ]\n",
        "\n",
        "        # removendo variáveis equivalentes phone_service=0\n",
        "        del churn_df[ \"multiple_lines=No phone service\" ]\n",
        "\n",
        "        # removendo variáveis codificada tornadas redundantes pelas deleções acima\n",
        "        del churn_df[ \"multiple_lines=No\"    ]\n",
        "        del churn_df[ \"online_security=No\"   ]\n",
        "        del churn_df[ \"online_backup=No\"     ]\n",
        "        del churn_df[ \"device_protection=No\" ]\n",
        "        del churn_df[ \"tech_support=No\"      ]\n",
        "        del churn_df[ \"streaming_tv=No\"      ]\n",
        "        del churn_df[ \"streaming_movies=No\"  ]\n",
        "        del churn_df[ \"internet_service=No\"  ]        \n",
        "        \n",
        "        new_column_names = {\n",
        "            'multiple_lines=Yes'                       : 'multiple_lines'\n",
        "        ,   'internet_service=DSL'                     : 'dsl'\n",
        "        ,   'internet_service=Fiber optic'             : 'fiber_optic'\n",
        "        ,   'online_security=Yes'                      : 'online_security'\n",
        "        ,   'online_backup=Yes'                        : 'online_backup'\n",
        "        ,   'device_protection=Yes'                    : 'device_protection'\n",
        "        ,   'tech_support=Yes'                         : 'tech_support'\n",
        "        ,   'streaming_tv=Yes'                         : 'streaming_tv'\n",
        "        ,   'streaming_movies=Yes'                     : 'streaming_movies'\n",
        "        ,   'contract=Month-to-month'                  : 'monthly_contract'\n",
        "        ,   'contract=One year'                        : 'one_year_contract'\n",
        "        ,   'contract=Two year'                        : 'two_year_contract'\n",
        "        ,   'payment_method=Bank transfer (automatic)' : 'bank_transfer'\n",
        "        ,   'payment_method=Credit card (automatic)'   : 'credit_card'\n",
        "        ,   'payment_method=Electronic check'          : 'electronic_check'\n",
        "        ,   'payment_method=Mailed check'              : 'mailed_check'\n",
        "        }\n",
        "        churn_df.rename(columns=new_column_names, inplace=True)                \n",
        "        util.report_df(LOGGER, churn_df)\n",
        "        return churn_df\n",
        "    \n",
        "    def engineer_features(self, churn_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        LOGGER.info('engineering new features')\n",
        "        np.random.seed(self.seed)\n",
        "        churn_df = churn_df.copy()\n",
        "        \n",
        "        # o primeiro quartil da variável tenure conforme análise univariada anterior.\n",
        "        # a probabilidade de rotatividade é inversamente proporcional à variável tenure\n",
        "        tenure_1st_quartile = churn_df['tenure'].quantile(0.25)\n",
        "        \n",
        "        # o terceiro quartil da variável monthly_charges conforme análise univariada anterior.\n",
        "        # a probabilidade de rotatividade é  proporcional à variável monthly_charges\n",
        "        charges_3rd_quartile = churn_df['monthly_charges'].quantile(0.75)\n",
        "\n",
        "        # quantidade de linhas necesário para criação do ruído\n",
        "        rows, cols = churn_df.shape\n",
        "        \n",
        "        LOGGER.info('creating client factor')\n",
        "        # client factor\n",
        "        # A análise das tabulações cruzadas revelou que a existência de parceiro e dependentes tendem a fidelizar o cliente.\n",
        "        # Em contrapartida, observou-se que clientes na terceira idade proporcionalmente tendem a cancelar os serviços\n",
        "        # de maneira mais frequente.\n",
        "        # A expectativa é de que quanto maior for o client_factor, maior a probabilidade de que ele venha a cancelar o seu contrato\n",
        "        noise_term = np.random.normal(loc=0.0, scale=self.NOISE_STD, size=rows)\n",
        "        churn_df[\"client_factor\"] = ((\n",
        "            np.exp(churn_df[\"senior_citizen\"]) # senior_citizen=1 aumenta a rotatividade\n",
        "        +   np.exp(np.abs(1-churn_df[\"partner\"]))\n",
        "        +   np.exp(np.abs(1-churn_df[\"dependents\"]))\n",
        "        +   np.exp((churn_df[\"tenure\"] < tenure_1st_quartile).astype(float))\n",
        "        +   np.exp((churn_df[\"monthly_charges\"] > charges_3rd_quartile).astype(float))\n",
        "        ) / 3.0 + noise_term) * (1.0 if self.ENABLE_FEATURE_ENGINEERING else 0.0)\n",
        "        \n",
        "        LOGGER.info('creating internet factor')\n",
        "        # internet factor\n",
        "        # A análise das tabulações cruzadas revelou que a existência a contratação dos\n",
        "        # serviços de suporte técnico e de segurança online tendem a indicar que um usuário\n",
        "        # encontra-se fidelizado. A contratação da internet de fibra ótica, ao elevar o valor\n",
        "        # mensalmente cobrado, contribui com a rotatividade do cliente. Por outro lado,\n",
        "        # os clientes com internet DSL tendem a permanecer como cliente devido a valor \n",
        "        # comparativamente mais baixo sendo cobrado.\n",
        "        # A expectativa é de que quanto maior for o internet_factor, maior a probabilidade de que ele venha a cancelar o seu contrato\n",
        "        noise_term = np.random.normal(loc=0.0, scale=self.NOISE_STD, size=rows)\n",
        "        churn_df[\"internet_factor\"] = ((\n",
        "            np.exp(np.abs(1-churn_df[\"tech_support\"])) \n",
        "        +   np.exp(np.abs(1-churn_df[\"online_security\"]))  \n",
        "        +   np.exp(churn_df[\"fiber_optic\"]) # senior_citizen=1 aumenta a rotatividade\n",
        "        -   np.exp(churn_df[\"dsl\"]) # dsl=1 diminui a rotatividade\n",
        "        +   np.exp((churn_df[\"tenure\"] < tenure_1st_quartile).astype(float))\n",
        "        +   np.exp((churn_df[\"monthly_charges\"] > charges_3rd_quartile).astype(float))\n",
        "        ) / 3.0 + noise_term) * (1.0 if self.ENABLE_FEATURE_ENGINEERING else 0.0)\n",
        "        \n",
        "        LOGGER.info('creating financial factor')\n",
        "        # financial factor\n",
        "        # A análise das tabulações cruzadas revelou que a existência o uso de cobrança digital,\n",
        "        # o uso de contratos mensais e o pagamento via cheque eletrônico são fatores que\n",
        "        # contribuem com a rotatividade dos clientes\n",
        "        # A expectativa é de que quanto maior for o financial_factor, maior a probabilidade de que ele venha a cancelar o seu contrato\n",
        "        noise_term = np.random.normal(loc=0.0, scale=self.NOISE_STD, size=rows)\n",
        "        churn_df[\"financial_factor\"] = ((\n",
        "            np.exp(churn_df[\"monthly_contract\"]) \n",
        "        +   np.exp(churn_df[\"electronic_check\"])\n",
        "        +   np.exp(churn_df[\"paperless_billing\"])\n",
        "        +   np.exp((churn_df[\"tenure\"] < tenure_1st_quartile).astype(float))\n",
        "        +   np.exp((churn_df[\"monthly_charges\"] > charges_3rd_quartile).astype(float))\n",
        "        ) / 3.0 + noise_term) * (1.0 if self.ENABLE_FEATURE_ENGINEERING else 0.0)\n",
        "        \n",
        "        LOGGER.info('combining factors into one')\n",
        "        # por último, criamos um fator combinando todos usados anteriormente\n",
        "        noise_term = np.random.normal(loc=0.0, scale=self.NOISE_STD, size=rows)\n",
        "        churn_df[\"multi_factor\"] = ((\n",
        "            np.exp(churn_df[\"senior_citizen\"]) # senior_citizen=1 piora as p\n",
        "        +   np.exp(np.abs(1-churn_df[\"partner\"]))\n",
        "        +   np.exp(np.abs(1-churn_df[\"dependents\"]))\n",
        "        +   np.exp(np.abs(1-churn_df[\"tech_support\"])) \n",
        "        +   np.exp(np.abs(1-churn_df[\"online_security\"])) \n",
        "        -   np.exp(churn_df[\"dsl\"]) \n",
        "        +   np.exp(churn_df[\"fiber_optic\"])\n",
        "        +   np.exp(churn_df[\"monthly_contract\"]) \n",
        "        +   np.exp(churn_df[\"electronic_check\"])\n",
        "        +   np.exp(churn_df[\"paperless_billing\"])\n",
        "        +   np.exp((churn_df[\"tenure\"] < tenure_1st_quartile).astype(float))\n",
        "        +   np.exp((churn_df[\"monthly_charges\"] > charges_3rd_quartile).astype(float))\n",
        "        ) / 9.0 + noise_term) * (1.0 if self.ENABLE_FEATURE_ENGINEERING else 0.0)\n",
        "\n",
        "        # reposiciona a variável target ao final\n",
        "        churn_df[\"churn\"] = churn_df.pop(\"churn\")\n",
        "        util.report_df(LOGGER, churn_df)\n",
        "        return churn_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbsCCyWz8twO",
        "outputId": "96496a5d-cf41-4d96-f4b3-d5089a1e28aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/feature_ranker.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/feature_ranker.py\n",
        "# -*- coding: utf-8 -*-\n",
        "import abc\n",
        "import argparse\n",
        "from typing import Tuple, List\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import telchurn.util as util\n",
        "\n",
        "\n",
        "from telchurn.data_loader import DataLoader\n",
        "\n",
        "LOGGER = util.get_logger('feature_ranker')\n",
        "\n",
        "class FeatureRanker(abc.ABC):\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def rank_features(self, df: pd.DataFrame, target_variable: str) -> Tuple[str, float]:\n",
        "        raise NotImplementedError\n",
        "    \n",
        "class FeatureRankerImpl(FeatureRanker):\n",
        "    \n",
        "    N_ESTIMATORS = 150\n",
        "    \n",
        "    def __init__(self, random_seed=None):\n",
        "        self.random_seed = random_seed\n",
        "\n",
        "    def rank_features(self, df: pd.DataFrame, target_variable: str) -> List[Tuple[str, float]]:\n",
        "        LOGGER.info('ranking features')\n",
        "        all_but_target = df.columns.difference([target_variable])\n",
        "        X_df = df[all_but_target]\n",
        "        y = df[target_variable]\n",
        "\n",
        "        classifier = RandomForestClassifier(\n",
        "            n_estimators  = self.N_ESTIMATORS\n",
        "        ,   bootstrap     = True\n",
        "        ,   class_weight  = \"balanced_subsample\"\n",
        "        ,   random_state  = self.random_seed\n",
        "        )\n",
        "        LOGGER.info('fitting random forest classifier')\n",
        "        classifier.fit(X_df, y)\n",
        "        \n",
        "        importances_df = pd.DataFrame({\n",
        "            \"feature\"       : X_df.columns\n",
        "        ,   \"importance\"    : classifier.feature_importances_\n",
        "        })\n",
        "        importances_df.sort_values(\"importance\", ascending=False, inplace=True)\n",
        "        result = list([\n",
        "            (row.feature, row.importance) for row in importances_df.itertuples()\n",
        "        ])\n",
        "        LOGGER.info('Ranked Features')\n",
        "        for feature, importance in result:\n",
        "            LOGGER.info(f'\\t-> {feature}: {importance}')\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDQgeJR880U1",
        "outputId": "aec9b1ab-565c-4f0a-b63b-55d93599e202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing telchurn/feature_selector.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile telchurn/feature_selector.py\n",
        "  # -*- coding: utf-8 -*-\n",
        "import abc\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import telchurn.util as util\n",
        "from telchurn.feature_ranker import FeatureRanker\n",
        "from telchurn.data_loader import DataLoader\n",
        "\n",
        "LOGGER = util.get_logger('feature_selector')\n",
        "\n",
        "class FeatureSelector(abc.ABC):\n",
        "       \n",
        "    @abc.abstractmethod\n",
        "    def select_features(self, top_k: int, target_variable: str, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "class FeatureSelectorImpl(abc.ABC):\n",
        "        \n",
        "    def __init__(self, feature_ranker: FeatureRanker):\n",
        "        self.feature_ranker = feature_ranker\n",
        "        \n",
        "    def select_features(self, top_k: int, target_variable: str, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        LOGGER.info(f'selecting top {top_k} features')\n",
        "        df = df.copy()\n",
        "        rankings = self.feature_ranker.rank_features(df, target_variable)\n",
        "        feature_names = []\n",
        "        for i in range(top_k):\n",
        "            feature_name, importance = rankings[i]\n",
        "            feature_names.append(feature_name)\n",
        "        if target_variable not in feature_names:\n",
        "            feature_names.append(target_variable)\n",
        "        df = df[ feature_names ].copy()\n",
        "        return df\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaMvpCQL9A8F",
        "outputId": "50d1ef2f-70ca-4a06-adb3-373e31845eea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/data_cleaner.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/data_cleaner.py\n",
        " # -*- coding: utf-8 -*-\n",
        "import io\n",
        "import sys\n",
        "import abc\n",
        "import pandas as pd\n",
        "import telchurn.util as util\n",
        "from telchurn.data_loader import DataLoader\n",
        "from telchurn.feature_processor import FeatureProcessor\n",
        "from telchurn.feature_selector import FeatureSelector\n",
        "\n",
        "\n",
        "LOGGER = util.get_logger('data_cleaner')\n",
        "\n",
        "class DataCleaner(abc.ABC):\n",
        "    \n",
        "    DEFAULT_TOP_K_FEATURES  = 16\n",
        "    DEFAULT_SEED = 42\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def clean(self, input_file_name_or_url: str, output_file_name_or_url: str, top_k_features: int=None, fields:str=None) -> None:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "class DataCleanerImpl(DataCleaner):\n",
        "    \n",
        "    TARGET_VARIABLE = \"churn\"\n",
        "    \n",
        "    def __init__(self, data_loader: DataLoader, feature_processor: FeatureProcessor, feature_selector: FeatureSelector):\n",
        "        self.data_loader = data_loader\n",
        "        self.feature_processor = feature_processor\n",
        "        self.feature_selector = feature_selector\n",
        "        \n",
        "    def clean(self, input_file_name_or_url: str, output_file_name_or_url: str, top_k_features: int=None, fields:str=None) -> None:\n",
        "        if top_k_features is None:\n",
        "            top_k_features = self.DEFAULT_TOP_K_FEATURES        \n",
        "        LOGGER.info(f'starting data cleaner')\n",
        "        churn_df = self.data_loader.load(input_file_name_or_url)\n",
        "        \n",
        "        # transforma a variável target em uma variável numérica\n",
        "        churn_df[\"churn\"] = churn_df[\"churn\"].map({\"No\": 0, \"Yes\": 1})\n",
        "\n",
        "        # excluindo a variável customer_id\n",
        "        del churn_df[\"customer_id\"]\n",
        "        \n",
        "        # coluna total_charges possui registros vazios com valor ' '\n",
        "        def convert_total_charges(value):\n",
        "            return 0.0 if value == ' ' else value\n",
        "        churn_df[\"total_charges\"] = churn_df[\"total_charges\"].map(convert_total_charges).astype(float)\n",
        "        \n",
        "        # diferente das outras colunas, senior_citizem possui valores 1 ou 0 ao invés de \"Yes\" or \"No\"\n",
        "        churn_df[\"senior_citizen\"] = churn_df[\"senior_citizen\"].map({1: \"Yes\", 0: \"No\"})\n",
        "        \n",
        "        # removendo a feature de sexo que se mostrou irrelevante durante a análise exploratória\n",
        "        del churn_df[\"gender\"]\n",
        "        \n",
        "        churn_df = self.feature_processor.handle_categorical_features(churn_df)\n",
        "        churn_df = self.feature_processor.engineer_features(churn_df)\n",
        "        if fields is None:\n",
        "            churn_df = self.feature_selector.select_features(top_k_features, self.TARGET_VARIABLE, churn_df)\n",
        "        else:\n",
        "            column_names = fields.split(',')\n",
        "            churn_df = churn_df[column_names]\n",
        "        self.save_cleansed(output_file_name_or_url, churn_df)\n",
        "    \n",
        "    def save_cleansed(self, file_name_or_url: str, churn_df: pd.DataFrame) -> None:\n",
        "        LOGGER.info(f'saving cleansed dataframe to {file_name_or_url}')\n",
        "        churn_df.to_csv(file_name_or_url, sep=DataLoader.DELIMITER, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBWQFG5I9N_z",
        "outputId": "dd9d873e-aa40-4746-c58c-d1a4367333d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/pipeline_factory.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/pipeline_factory.py\n",
        " # -*- coding: utf-8 -*-\n",
        "import io\n",
        "import sys\n",
        "import abc\n",
        "import pandas as pd\n",
        "import telchurn.util as util\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "class PipelineFactory(abc.ABC):\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def build_pipeline_for(self, churn_df: pd.DataFrame) -> Pipeline:\n",
        "        raise NotImplementedError\n",
        "\n",
        "class PipelineFactoryImpl(PipelineFactory):\n",
        "    \n",
        "    NUMERICAL_FEATURES = [\n",
        "        'tenure'\n",
        "    ,   'monthly_charges'\n",
        "    ,   'total_charges'\n",
        "    ,   'client_factor'\n",
        "    ,   'internet_factor'\n",
        "    ,   'financial_factor'\n",
        "    ,   'multi_factor'    \n",
        "    ]\n",
        "    \n",
        "    def select_numerical_features(self, churn_df):\n",
        "        # seleciona as variáveis numéricas que existem no data frame\n",
        "        return list([nf for nf in self.NUMERICAL_FEATURES if nf in churn_df.columns])\n",
        "        \n",
        "    def build_pipeline_for(self, churn_df: pd.DataFrame) -> Pipeline:\n",
        "        numerical_features = self.select_numerical_features(churn_df)\n",
        "        # Configuração do pipeline\n",
        "\n",
        "        # Os transformadores numéricos são utilizado spara processamento de todas as variáveis não categóricas.\n",
        "        numeric_transformer = Pipeline([\n",
        "          (\"scaler\", StandardScaler())    \n",
        "        ])\n",
        "\n",
        "        column_transformer = ColumnTransformer(\n",
        "          transformers = [\n",
        "            (\"num\", numeric_transformer, numerical_features)\n",
        "          ],\n",
        "          # importante usar passthrough quando nem todos os atributos forem processados\n",
        "          remainder=\"passthrough\" \n",
        "        )\n",
        "\n",
        "        # Este pipelie será ajustado diversas vezes durante o processo de otimização dos hiper parâmetros.\n",
        "        pipeline = Pipeline([\n",
        "            # a primeira fase consiste no pré-processamento das variáveis numéricas\n",
        "            (\"feature_scaling\", column_transformer),\n",
        "            # redução de dimesionalidade\n",
        "            (\"reduce_dim\", PCA()),\n",
        "            # O algoritmo de regressão e seus parâmetros serão configurados via gridsearch\n",
        "            (\"classifier\", SVC())\n",
        "        ])\n",
        "        return pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3zqOWjp92Ah",
        "outputId": "31993200-ba82-4961-86be-f081f2a2ef09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/hyper_param_tunner.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/hyper_param_tunner.py\n",
        " # -*- coding: utf-8 -*-\n",
        "import io\n",
        "import os\n",
        "import sys\n",
        "import abc\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import telchurn.util as util\n",
        "from typing import List, Dict, Tuple\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "LOGGER = util.get_logger('hp_tunner')\n",
        "\n",
        "class HyperParamTunner(abc.ABC):\n",
        "    \n",
        "    DEFAULT_K_FOLDS = 10\n",
        "    DEFAULT_METRIC  = \"f1\"\n",
        "    \n",
        "    def __init__(self, k_folds, random_seed=None):\n",
        "        self.kfold = StratifiedKFold(\n",
        "            n_splits      = k_folds\n",
        "        ,   shuffle       = True\n",
        "        ,   random_state  = random_seed\n",
        "        )     \n",
        "        \n",
        "    @abc.abstractmethod\n",
        "    def tune(self, pipeline: Pipeline, param_grid: List[Dict], grid_name: str, scoring_metric: str, X_train_df: pd.DataFrame, y_train_df: pd.DataFrame) -> Tuple[RandomizedSearchCV, pd.DataFrame]:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "class HyperParamTunnerImpl(HyperParamTunner):\n",
        "    \n",
        "    def __init__(self, k_folds, random_seed=None):\n",
        "        self.kfold = StratifiedKFold(\n",
        "            n_splits      = k_folds\n",
        "        ,   shuffle       = True\n",
        "        ,   random_state  = random_seed\n",
        "        )     \n",
        "                \n",
        "    def tune(self, pipeline: Pipeline, param_grid: List[Dict], grid_name: str, num_iterations: int, scoring_metric: str, X_train_df: pd.DataFrame, y_train_df: pd.DataFrame) -> Tuple[RandomizedSearchCV, pd.DataFrame]:\n",
        "        LOGGER.info(f'tunning pipeline {grid_name} using scoring metric {scoring_metric}')\n",
        "        grid = RandomizedSearchCV(\n",
        "            estimator           = pipeline\n",
        "        ,   param_distributions = param_grid\n",
        "        ,   scoring             = scoring_metric\n",
        "        ,   cv                  = self.kfold\n",
        "        ,   n_iter              = num_iterations\n",
        "        ,   return_train_score  = False\n",
        "        ,   n_jobs              = -1\n",
        "        )\n",
        "        warnings.filterwarnings(\"ignore\")\n",
        "        util.silence_warnings()\n",
        "        grid.fit(X_train_df, y_train_df)\n",
        "        LOGGER.info(f'Best {scoring_metric}: {grid.best_score_}')\n",
        "        LOGGER.info(f'Best estimator: -> \\n{grid.best_estimator_}')\n",
        "        results_df = pd.DataFrame(grid.cv_results_)\n",
        "        return grid, results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPFAdxao990c",
        "outputId": "c9331944-08a7-41d9-a4fc-7341b22b3634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/model_repository.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/model_repository.py\n",
        "# -*- coding: utf-8 -*-\n",
        "import abc\n",
        "import os.path\n",
        "import glob\n",
        "import pickle\n",
        "from typing import Tuple, List, Dict\n",
        "import pandas as pd\n",
        "import telchurn.util as util\n",
        "# correção de bug na biblioteca que importa uma dependência de maneira indireta\n",
        "# https://stackoverflow.com/questions/61867945/python-import-error-cannot-import-name-six-from-sklearn-externals\n",
        "import sys\n",
        "import six\n",
        "sys.modules['sklearn.externals.six'] = six\n",
        "from mlxtend.classifier import EnsembleVoteClassifier\n",
        "\n",
        "LOGGER = util.get_logger('model_repository')\n",
        "\n",
        "class ModelRepository(abc.ABC):\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def save_grid(self, grid: Dict, file_name: str) -> None:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def load_grid(self, file_name: str) -> Dict:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def save_final_model(self, model: EnsembleVoteClassifier, file_name: str) -> None:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def load_final_model(self, file_name: str) -> EnsembleVoteClassifier:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "class ModelRepositoryImpl(ModelRepository):\n",
        "    \n",
        "    GRID_PREFIX = \"grid_\"\n",
        "    GRID_GLOB = \"grid_*.pkl\"\n",
        "    \n",
        "    def __init__(self, repo_dir: str):\n",
        "        self.repo_dir = repo_dir\n",
        "    \n",
        "    def list_grids(self):\n",
        "        LOGGER.info(f'listing saved grids on {self.repo_dir}')\n",
        "        path = os.path.join(self.repo_dir, self.GRID_GLOB)\n",
        "        def get_grid_name(path):\n",
        "            # f = lambda path: os.path.split(path)[-1].replace(self.GRID_PREFIX, \"\") # eita, gambiarra danada!\n",
        "            path_parts = os.path.split(path)\n",
        "            filename = path_parts[-1]\n",
        "            filename = filename.replace(self.GRID_PREFIX, \"\")\n",
        "            return filename\n",
        "        return list([ get_grid_name(path) for path in glob.glob(path) ])\n",
        "        \n",
        "    def save_grid(self, grid: Dict, file_name: str) -> None:\n",
        "        path = os.path.join(self.repo_dir, self.GRID_PREFIX + file_name)\n",
        "        LOGGER.info(f'saving grid search results to {path}')\n",
        "        data = {\n",
        "            \"best_score_\"     : grid.best_score_\n",
        "        ,   \"best_params_\"    : grid.best_params_\n",
        "        ,   \"best_estimator_\" : grid.best_estimator_\n",
        "        ,   \"cv_results_\"     : grid.cv_results_\n",
        "        ,   \"grid\"            : grid\n",
        "        }  \n",
        "        with open(path, \"wb\") as fh:\n",
        "            pickle.dump(data, fh)\n",
        "        \n",
        "    def load_grid(self, file_name: str) -> Dict:\n",
        "        path = os.path.join(self.repo_dir, self.GRID_PREFIX + file_name)\n",
        "        LOGGER.info(f'loading grid search results from {path}')\n",
        "        with open(path, \"rb\") as fh:\n",
        "            data                  = pickle.load(fh)\n",
        "        grid                  = data[\"grid\"]\n",
        "        grid.best_score_      = data[\"best_score_\"]\n",
        "        grid.best_params_     = data[\"best_params_\"]\n",
        "        grid.best_estimator_  = data[\"best_estimator_\"]\n",
        "        grid.cv_results_      = data[\"cv_results_\"]\n",
        "        return grid\n",
        "    \n",
        "    def save_final_model(self, model: EnsembleVoteClassifier, file_name: str) -> None:\n",
        "        path = os.path.join(self.repo_dir, file_name)\n",
        "        LOGGER.info(f'saving final model to {path}')\n",
        "        with open(path, \"wb\") as fh:\n",
        "            pickle.dump(model, fh)\n",
        "\n",
        "    def load_final_model(self, file_name: str) -> EnsembleVoteClassifier:\n",
        "        path = os.path.join(self.repo_dir, file_name)\n",
        "        LOGGER.info(f'loading final model from {path}')\n",
        "        with open(path, \"rb\") as fh:\n",
        "            return pickle.load(fh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMGmYecb-GeL",
        "outputId": "bd8c2be9-6087-49d2-dd72-3bad033c76fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/param_grids.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/param_grids.py\n",
        " # -*- coding: utf-8 -*-\n",
        "import abc\n",
        "import pandas as pd\n",
        "import telchurn.util as util\n",
        "from typing import List, Dict\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve, plot_roc_curve\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer # transformador de colunas, usado para tratamento das variáveis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "LOGGER = util.get_logger('param_grids')\n",
        "\n",
        "QUICK_RUN   = False\n",
        "USE_LOGREG  = True\n",
        "USE_KNN     = True\n",
        "USE_NB      = True\n",
        "USE_DT      = True\n",
        "USE_SVM     = False\n",
        "USE_ADA     = True\n",
        "USE_GB      = True\n",
        "USE_RF      = True\n",
        "\n",
        "\n",
        "\n",
        "class ParamGrids(abc.ABC):\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def get_parameter_grids(self) -> List[Dict]:\n",
        "        raise NotImplementedError\n",
        "    \n",
        "class ParamGridsImpl(abc.ABC):\n",
        "    \n",
        "    def get_parameter_grids(self) -> List:\n",
        "        return []                   \\\n",
        "        +   (self.get_logreg_grid() if USE_LOGREG else []) \\\n",
        "        +   (self.get_knn_grid()    if USE_KNN else []) \\\n",
        "        +   (self.get_nb_grid()     if USE_NB  else []) \\\n",
        "        +   (self.get_dt_grid()     if USE_DT  else []) \\\n",
        "        +   (self.get_svm_grid()    if USE_SVM else []) \\\n",
        "        +   (self.get_ada_grid()    if USE_ADA else []) \\\n",
        "        +   (self.get_gb_grid()     if USE_GB  else []) \\\n",
        "        +   (self.get_rf_grid()     if USE_RF  else [])\n",
        "        \n",
        "    def get_logreg_grid(self):\n",
        "        LOGGER.info('creating parameter grids for logistic Regression - LOGREG')\n",
        "        param_grid = [{\n",
        "            # Logistic Regression\n",
        "            \"feature_scaling__num__scaler\"  : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "            \"reduce_dim\"                    : [\"passthrough\", PCA(n_components=3), PCA(n_components=5)],\n",
        "            \"classifier\"                    : [LogisticRegression()],\n",
        "            \"classifier__n_jobs\"            : [-1], # all cpus available\n",
        "            \"classifier__penalty\"           : [\"elasticnet\"],\n",
        "            \"classifier__class_weight\"      : [\"balanced\"],\n",
        "            \"classifier__solver\"            : [\"saga\"],\n",
        "            \"classifier__l1_ratio\"          : [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "        },\n",
        "        {\n",
        "            \"feature_scaling__num__scaler\"  : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "            \"reduce_dim\"                    : [\"passthrough\", PCA(n_components=3), PCA(n_components=5)],\n",
        "            \"classifier\"                    : [LogisticRegression()],\n",
        "            \"classifier__n_jobs\"            : [-1], # all cpus available\n",
        "            \"classifier__penalty\"           : [\"none\"],\n",
        "            \"classifier__class_weight\"      : [\"balanced\"],\n",
        "        }]\n",
        "        return [{\n",
        "            \"name\"          : \"LOGREG\"\n",
        "        ,   \"iterations\"    : (200 if not QUICK_RUN else 20)\n",
        "        ,   \"param_grid\"    : param_grid\n",
        "        }]\n",
        "    \n",
        "    def get_knn_grid(self):\n",
        "        LOGGER.info('creating parameter grids for K Nearest Neighbors - KNN')\n",
        "        param_grid = [{\n",
        "            # KNeighborsClassifier\n",
        "            \"feature_scaling__num__scaler\"  : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "            \"reduce_dim\"                    : [\"passthrough\", PCA(n_components=3), PCA(n_components=5)],\n",
        "            \"classifier\"                    : [KNeighborsClassifier()],\n",
        "            \"classifier__n_jobs\"            : [-1], # all cpus available\n",
        "            \"classifier__algorithm\"         : [\"kd_tree\"],\n",
        "            \"classifier__metric\"            : [\"minkowski\"],\n",
        "            \"classifier__p\"                 : [0.5, 1.0, 1.5, 2.0],\n",
        "            \"classifier__n_neighbors\"       : [5, 7, 10, 13, 15, 17, 20],\n",
        "            \"classifier__weights\"           : [\"uniform\", \"distance\"]\n",
        "            }]\n",
        "        return [{\n",
        "            \"name\"          : \"KNN\"\n",
        "        ,   \"iterations\"    : (200 if not QUICK_RUN else 20)\n",
        "        ,   \"param_grid\"    : param_grid\n",
        "        }]\n",
        "    \n",
        "    def get_nb_grid(self):\n",
        "        LOGGER.info('creating parameter grids for Naive Bayes - nb')\n",
        "        param_grid = [{\n",
        "            # GaussianNB\n",
        "            \"feature_scaling__num__scaler\"  : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "            \"reduce_dim\"                    : [\"passthrough\", PCA(n_components=3), PCA(n_components=5)],\n",
        "            \"classifier\"                    : [GaussianNB()],\n",
        "            \"classifier__var_smoothing\"     : [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
        "        }]    \n",
        "        return [{\n",
        "            \"name\"          : \"NB\"\n",
        "        ,   \"iterations\"    : (200 if not QUICK_RUN else 20)\n",
        "        ,   \"param_grid\"    : param_grid\n",
        "        }]\n",
        "   \n",
        "    def get_dt_grid(self):\n",
        "        LOGGER.info('creating parameter grids for Decision Tree - DT')\n",
        "        param_grid = [{\n",
        "            # DecisionTreeClassifier{\n",
        "            \"feature_scaling__num__scaler\"  : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "            \"reduce_dim\"                    : [\"passthrough\", PCA(n_components=3), PCA(n_components=5)],\n",
        "            \"classifier\"                    : [DecisionTreeClassifier()],\n",
        "            \"classifier__class_weight\"      : [None, \"balanced\"],\n",
        "            \"classifier__criterion\"         : [\"gini\", \"entropy\"],\n",
        "            \"classifier__splitter\"          : [\"best\", \"random\"],\n",
        "            \"classifier__max_features\"      : [None, \"auto\", \"sqrt\", \"log2\"],\n",
        "            \"classifier__max_depth\"         : [10, 25, 50]\n",
        "        }]\n",
        "        return [{\n",
        "            \"name\"          : \"DT\"\n",
        "        ,   \"iterations\"    : (200 if not QUICK_RUN else 20)\n",
        "        ,   \"param_grid\"    : param_grid\n",
        "        }]\n",
        "\n",
        "    def get_svm_grid(self):\n",
        "        LOGGER.info('creating parameter grids for Suport Vector Machine classifier - SVM')\n",
        "        param_grid = param_grid = [{\n",
        "            # SVC        \n",
        "            \"feature_scaling__num__scaler\"  : [MinMaxScaler(), StandardScaler()], # SVC precisa ter os argumentos escalonados para uma melhor performance\n",
        "            #\"reduce_dim\"                    : [\"passthrough\", PCA(n_components=3), PCA(n_components=5)],\n",
        "            \"reduce_dim\"                    : [PCA(n_components=3), PCA(n_components=5), PCA(n_components=7)],\n",
        "            \"classifier\"                    : [SVC(probability=True)],\n",
        "            \"classifier__kernel\"            : [\"linear\",\"rbf\"],\n",
        "            \"classifier__gamma\"             : [\"scale\", \"auto\"],\n",
        "            \"classifier__class_weight\"      : [None, \"balanced\"],\n",
        "            \"classifier__tol\"               : [1e-2],\n",
        "            \"classifier__C\"                 : [1, 0.5, 0.1],\n",
        "            #\"classifier__max_iter\"          : [1000],\n",
        "\n",
        "        }]\n",
        "        return [{\n",
        "            \"name\"          : \"SVM\"\n",
        "        ,   \"iterations\"    : (200 if not QUICK_RUN else 20)\n",
        "        ,   \"param_grid\"    : param_grid\n",
        "        }]\n",
        "    \n",
        "    def get_ada_grid(self):\n",
        "        LOGGER.info('creating parameter grids for AdaBoost classifier - ADA')\n",
        "        param_grid = [{\n",
        "        # AdaBoostClassifier\n",
        "            \"feature_scaling__num__scaler\"    : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "            \"reduce_dim\"                      : [\"passthrough\", PCA(n_components=3), PCA(n_components=5)],\n",
        "            \"classifier\"                      : [AdaBoostClassifier()],\n",
        "            \"classifier__n_estimators\"        : [100, 250, 500],\n",
        "            \"classifier__learning_rate\"       : [0.001, 0.01, 0.1, 1.0]\n",
        "        }]\n",
        "        return [{\n",
        "            \"name\"          : \"ADA\"\n",
        "        ,   \"iterations\"    : (200 if not QUICK_RUN else 20)\n",
        "        ,   \"param_grid\"    : param_grid\n",
        "        }]\n",
        "\n",
        "    def get_gb_grid(self):\n",
        "        LOGGER.info('creating parameter grids for Gradient Boosting classifier - GB')\n",
        "        param_grid = [{\n",
        "            # GradientBoostingClassifier        \n",
        "            \"feature_scaling__num__scaler\"    : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "            \"reduce_dim\"                      : [\"passthrough\", PCA(n_components=3), PCA(n_components=5)],\n",
        "            \"classifier\"                      : [GradientBoostingClassifier()],\n",
        "            \"classifier__loss\"                : [\"log_loss\", \"deviance\", \"exponential\"],\n",
        "            \"classifier__n_estimators\"        : [50, 75, 100, 150],\n",
        "            \"classifier__learning_rate\"       : [0.1, 0.3, 0.5, 0.7, 1.0],\n",
        "            \"classifier__max_depth\"           : [3, 5, 10],\n",
        "            \"classifier__max_features\"        : [None, \"sqrt\", \"log2\"]\n",
        "        }]\n",
        "        return [{\n",
        "            \"name\"          : \"GB\"\n",
        "        ,   \"iterations\"    : (200 if not QUICK_RUN else 20)\n",
        "        ,   \"param_grid\"    : param_grid\n",
        "        }]\n",
        "        \n",
        "    def get_rf_grid(self):\n",
        "        LOGGER.info('creating parameter grids for Random Forest classifier - RF')\n",
        "        param_grid = [{\n",
        "            # RandomForestClassifier\n",
        "            \"feature_scaling__num__scaler\"    : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "            \"reduce_dim\"                      : [\"passthrough\"], #PCA(n_components=5), PCA(n_components=10)],\n",
        "            \"classifier\"                      : [RandomForestClassifier()],\n",
        "            \"classifier__n_estimators\"        : [50, 100, 150, 200],\n",
        "            \"classifier__criterion\"           : [\"gini\", \"entropy\"],\n",
        "            \"classifier__bootstrap\"           : [True, False],\n",
        "            \"classifier__n_jobs\"              : [-1],\n",
        "            \"classifier__class_weight\"        : [\"balanced\", \"balanced_subsample\"],\n",
        "            \"classifier__max_depth\"           : [5, 10, 25]\n",
        "        }]\n",
        "        return [{\n",
        "            \"name\"          : \"RF\"\n",
        "        ,   \"iterations\"    : (200 if not QUICK_RUN else 20)\n",
        "        ,   \"param_grid\"    : param_grid\n",
        "        }]\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDtC0qrA-Pmh",
        "outputId": "edb4e3e6-56ab-447b-db63-72bd3bd1cc14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/trainer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/trainer.py\n",
        " # -*- coding: utf-8 -*-\n",
        "import abc\n",
        "import pandas as pd\n",
        "from typing import Tuple, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import telchurn.util as util\n",
        "from telchurn.data_loader import DataLoader\n",
        "from telchurn.pipeline_factory import PipelineFactory\n",
        "from telchurn.hyper_param_tunner import HyperParamTunner\n",
        "from telchurn.param_grids import ParamGridsImpl\n",
        "from telchurn.model_repository import ModelRepository\n",
        "from telchurn.data_splitter import DataSplitter\n",
        "\n",
        "LOGGER = util.get_logger('trainer')\n",
        "\n",
        "class Trainer(abc.ABC):\n",
        "    \n",
        "    #DEFAULT_TEST_PCT_SIZE   = 0.3 # 30% do conjunto de dados\n",
        "    #DEFAULT_RANDOM_STATE    = 42\n",
        "                \n",
        "    @abc.abstractmethod\n",
        "    def train(self, input_file: str, splitter: DataSplitter) -> None:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "class TrainerImpl(Trainer):\n",
        "    \n",
        "    #SCORING_METHOD = \"recall\"\n",
        "    SCORING_METHOD = \"balanced_accuracy\"\n",
        "    \n",
        "    def __init__(self, data_loader: DataLoader, pipeline_factory: PipelineFactory, hp_tunner: HyperParamTunner, repo: ModelRepository):\n",
        "        self.data_loader = data_loader\n",
        "        self.pipeline_factory = pipeline_factory\n",
        "        self.hp_tunner = hp_tunner\n",
        "        self.repo = repo\n",
        "    \n",
        "    def get_param_grids(self):\n",
        "        return ParamGridsImpl().get_parameter_grids()\n",
        "\n",
        "    def train(self, input_file: str, splitter: DataSplitter) -> None:\n",
        "        LOGGER.info('starting telco churn model training')\n",
        "        churn_df = self.data_loader.load_cleansed(input_file)\n",
        "        target = churn_df.columns[-1]\n",
        "        util.report_df(LOGGER, churn_df)\n",
        "        pipeline = self.pipeline_factory.build_pipeline_for(churn_df)\n",
        "        (X_train_df, y_train_df), (X_test_df, y_test_df) = splitter.split(churn_df, target)\n",
        "        param_grids = param_grids = self.get_param_grids()\n",
        "        for param_grid in param_grids:\n",
        "            name        = param_grid[\"name\"]\n",
        "            iterations  = param_grid[\"iterations\"]\n",
        "            grid        = param_grid[\"param_grid\"]\n",
        "            rand_search_cv, train_df = self.hp_tunner.tune(\n",
        "                pipeline        = pipeline\n",
        "            ,   param_grid      = grid\n",
        "            ,   grid_name       = name\n",
        "            ,   num_iterations  = iterations\n",
        "            ,   scoring_metric  = self.SCORING_METHOD\n",
        "            ,   X_train_df      = X_train_df\n",
        "            ,   y_train_df      = y_train_df\n",
        "            )\n",
        "            grid_name = name + \".pkl\"\n",
        "            self.repo.save_grid(rand_search_cv, grid_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUv_MNZP-ZpY",
        "outputId": "681882ff-836f-41e9-d7bc-0cd87f145638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/model_evaluator.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/model_evaluator.py\n",
        "# -*- coding: utf-8 -*-\n",
        "import abc\n",
        "import os.path\n",
        "import pickle\n",
        "import math\n",
        "from itertools import combinations\n",
        "from typing import Tuple, List, Dict\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, balanced_accuracy_score, f1_score, confusion_matrix\n",
        "from mlxtend.classifier import EnsembleVoteClassifier\n",
        "\n",
        "import telchurn.util as util\n",
        "\n",
        "LOGGER = util.get_logger('model_evaluator')\n",
        "\n",
        "class ModelEvaluator(abc.ABC):\n",
        "    \n",
        "    @abc.abstractmethod\n",
        "    def report_results(self, estimator, X, y):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "class ModelEvaluatorImpl(ModelEvaluator):\n",
        "    \n",
        "    def report_results(self, estimator: EnsembleVoteClassifier, X_df: pd.DataFrame, y_df: pd.DataFrame) -> None:\n",
        "        y_hat           = estimator.predict(X_df)\n",
        "        # Confusion matrix whose i-th row and j-th column entry indicates the number of \n",
        "        # samples with true label being i-th class and predicted label being j-th class.\n",
        "        accuracy        = accuracy_score(y_df, y_hat)\n",
        "        precision       = precision_score(y_df, y_hat)\n",
        "        recall          = recall_score(y_df, y_hat)\n",
        "        balanced_acc    = balanced_accuracy_score(y_df, y_hat)\n",
        "        f1              = f1_score(y_df, y_hat)\n",
        "        conf_matrix     = confusion_matrix(y_df, y_hat)\n",
        "        true_negative   = conf_matrix[0][0]\n",
        "        false_positive  = conf_matrix[0][1]\n",
        "        false_negative  = conf_matrix[1][0]\n",
        "        true_positive   = conf_matrix[1][1]\n",
        "        \n",
        "        LOGGER.info(f\"accuracy score        : {accuracy}\")\n",
        "        LOGGER.info(f\"precision score       : {precision}\")\n",
        "        LOGGER.info(f\"recall score          : {recall}\")\n",
        "        LOGGER.info(f\"balanced acc. score   : {balanced_acc}\")\n",
        "        LOGGER.info(f\"f1 score              : {f1}\")\n",
        "        LOGGER.info(f\"confusion matrix\") \n",
        "        LOGGER.info(f\"\\tTrue  Negative : {true_negative}\") \n",
        "        LOGGER.info(f\"\\tFalse Positive : {false_positive}\") \n",
        "        LOGGER.info(f\"\\tFalse Negative : {false_negative}\") \n",
        "        LOGGER.info(f\"\\tTrue  Positive : {true_positive}\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwzewIJU-jq9",
        "outputId": "8047dd68-7abd-4cdc-b7a0-6b8ec871001e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./telchurn/ensembler.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./telchurn/ensembler.py\n",
        "# -*- coding: utf-8 -*-\n",
        "import abc\n",
        "import os.path\n",
        "import pickle\n",
        "import math\n",
        "from itertools import combinations\n",
        "from typing import Tuple, List, Dict\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, balanced_accuracy_score, f1_score, confusion_matrix\n",
        "from mlxtend.classifier import EnsembleVoteClassifier\n",
        "from telchurn.data_splitter import DataSplitter\n",
        "from telchurn.model_evaluator import ModelEvaluator\n",
        "import telchurn.util as util\n",
        "\n",
        "LOGGER = util.get_logger('ensembler')\n",
        "\n",
        "class Ensembler(abc.ABC):\n",
        "        \n",
        "    @abc.abstractmethod\n",
        "    def ensemble_models(self, grids: List[RandomizedSearchCV], churn_df: pd.DataFrame) -> EnsembleVoteClassifier:\n",
        "        raise NotImplementedError\n",
        "            \n",
        "class EnsemblerImpl(Ensembler):\n",
        "    \n",
        "     # soft voting é aquele no qual o estimador com mais \"certeza\" sobre a classificação vence\n",
        "    MIN_ESTIMATORS  = 1\n",
        "\n",
        "    def __init__(self, splitter: DataSplitter, evaluator: ModelEvaluator):\n",
        "        self.splitter = splitter\n",
        "        self.evaluator = evaluator\n",
        "        self.top10_scores = [(0.0, 0, \"\")] * 10\n",
        "    \n",
        "    def update_scores(self, score, num_estimators, voting_type):\n",
        "        self.top10_scores.append((score, num_estimators, voting_type))\n",
        "        self.top10_scores.sort(reverse=True)\n",
        "        self.top10_scores.pop(10)\n",
        "        LOGGER.info(f\"current top scores\")\n",
        "        for i, (score, num_estimators, voting_type) in enumerate(self.top10_scores):\n",
        "            if num_estimators == 0:\n",
        "                continue\n",
        "            LOGGER.info(f\"\\t{i+1} - {score} with {num_estimators} estimators and {voting_type} voting\")\n",
        "    \n",
        "    def compute_estimator_weights(self, grids: List[RandomizedSearchCV]) -> List[Tuple[RandomizedSearchCV, float]]:\n",
        "        LOGGER.info('computing estimator weights')\n",
        "        result = list([ \n",
        "            #(grid.best_estimator_, grid.best_score_) \n",
        "            (grid.best_estimator_, math.exp(1.0+grid.best_score_))\n",
        "            for grid in grids \n",
        "        ])\n",
        "        result.sort(key=lambda x: x[1])\n",
        "        return result\n",
        "    \n",
        "    def compute_score(self, estimator, X_test_df, y_test):\n",
        "      y_test_hat = estimator.predict(X_test_df)\n",
        "      train_score = balanced_accuracy_score(y_test, y_test_hat)\n",
        "      return train_score\n",
        "            \n",
        "    def ensemble_models(self, grids: List[RandomizedSearchCV], churn_df: pd.DataFrame) -> EnsembleVoteClassifier:\n",
        "        target = churn_df.columns[-1]\n",
        "        (X_train_df, y_train_df), (X_test_df, y_test_df) = self.splitter.split(churn_df, target) \n",
        "        estimators_and_weights = self.compute_estimator_weights(grids)\n",
        "        total_estimators = len(estimators_and_weights)\n",
        "        assert self.MIN_ESTIMATORS <= total_estimators\n",
        "        num_combined_estimators = range(self.MIN_ESTIMATORS, total_estimators + 1) # final de range é não inclusivo\n",
        "        best_score          = -99999\n",
        "        best_estimator      = None\n",
        "        best_voting_type    = None\n",
        "        util.silence_warnings()\n",
        "        for voting_type in ['soft', 'hard']:\n",
        "            for num_estimators in num_combined_estimators:\n",
        "                for comb_estimators_weights in combinations(estimators_and_weights, num_estimators):\n",
        "                    # https://stackoverflow.com/questions/13635032/what-is-the-inverse-function-of-zip-in-python\n",
        "                    comb_estimators, weights = zip(*comb_estimators_weights)\n",
        "                    # em versões mais antigas da biblioteca, o parâmetro fit_base_estimators chamava-se refit\n",
        "                    classifier = EnsembleVoteClassifier(\n",
        "                        clfs                 = comb_estimators\n",
        "                    ,   weights              = weights\n",
        "                    ,   voting               = voting_type\n",
        "                    #,   fit_base_estimators = False\n",
        "                    ,   refit                = False\n",
        "                    )\n",
        "                    classifier.fit(None, y_train_df) # nenhum dado é necessário pois fit_base_estimators=False\n",
        "                    score = self.compute_score(classifier, X_test_df, y_test_df)\n",
        "                    if score > best_score:\n",
        "                        best_score          = score\n",
        "                        best_estimator      = classifier\n",
        "                        best_voting_type    = voting_type\n",
        "                        self.update_scores(best_score,  num_estimators, best_voting_type)\n",
        "                    \n",
        "        LOGGER.info(f\"best combination of estimators: \")\n",
        "        for clf in best_estimator.clfs:\n",
        "            LOGGER.info(f\"\\t{clf}\")\n",
        "        LOGGER.info(f\"estimators weights: {classifier.weights}\")\n",
        "        LOGGER.info(\"Train Results\")\n",
        "        self.evaluator.report_results(best_estimator, X_train_df, y_train_df)\n",
        "        LOGGER.info(\"Test Results\")\n",
        "        self.evaluator.report_results(best_estimator, X_test_df, y_test_df)\n",
        "        \n",
        "        LOGGER.info('refiting base classifiers on whole data set')\n",
        "        X = pd.concat([X_train_df, X_test_df])\n",
        "        y = pd.concat([y_train_df, y_test_df])\n",
        "        for clf in best_estimator.clfs:\n",
        "            clf.fit(X, y)\n",
        "        best_estimator.fit(None, y) # nenhum dado é necessário pois fit_base_estimators=False\n",
        "        LOGGER.warn(\"Whole data set results (has data leakage)\")\n",
        "        self.evaluator.report_results(best_estimator, X, y)\n",
        "        return best_estimator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVp2Tds4_BXW",
        "outputId": "697e14d4-7860-4066-d949-5d3e2dbeb0d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./splitter.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./splitter.py\n",
        "# -*- coding: utf-8 -*-\n",
        "import abc\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from telchurn.data_loader import DataLoader, DataLoaderImpl\n",
        "from telchurn.data_splitter import DataSplitter, DataSplitterImpl\n",
        "import telchurn.util as util\n",
        "\n",
        "LOGGER = util.get_logger('splitter')\n",
        "\n",
        "def main(seed: int, testsplit: float, input_file: str, output_file1: str, output_file2: str) -> None:\n",
        "    data_loader = DataLoaderImpl()\n",
        "    data_splitter = DataSplitterImpl(seed, testsplit)\n",
        "    LOGGER.info('starting data splitter')\n",
        "    df = data_loader.load_cleansed(input_file) # GAMBIARRA: load cleansed does not alter the input file\n",
        "    LOGGER.info(f\"input file: {input_file}\")\n",
        "    util.report_df(LOGGER, df)\n",
        "    target = df.columns[-1]\n",
        "    LOGGER.info(f'using column \"{target}\" as target variable')\n",
        "    (X_train_df, y_train_df), (X_test_df, y_test_df) = data_splitter.split(df, target)\n",
        "    \n",
        "    train_df = X_train_df.copy()\n",
        "    train_df[target] = y_train_df[target].values\n",
        "    train_df = train_df[ df.columns ] # reorder columns - don't know why this is needed\n",
        "    \n",
        "    test_df = X_test_df.copy()\n",
        "    test_df[target] = y_test_df[target].values\n",
        "    test_df = test_df[ df.columns ] # reorder columns - don't know why this is needed\n",
        "    \n",
        "    LOGGER.info(f\"output file 1: {output_file1}\")\n",
        "    util.report_df(LOGGER, train_df)\n",
        "    LOGGER.info(f\"output file 2: {output_file2}\")\n",
        "    util.report_df(LOGGER, test_df)\n",
        "    \n",
        "    train_df.to_csv(output_file1, sep=DataLoader.DELIMITER, index=False)\n",
        "    test_df.to_csv(output_file2, sep=DataLoader.DELIMITER, index=False)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--seed', type=int,   help='random seed', default=DataSplitter.DEFAULT_RANDOM_STATE)\n",
        "    parser.add_argument('--testsplit', type=float, help='test split percentage', default=DataSplitter.DEFAULT_TEST_PCT_SIZE)\n",
        "    parser.add_argument('input_file', type=str,   help='input file name')\n",
        "    parser.add_argument('output_file1', type=str,   help='output file 1')\n",
        "    parser.add_argument('output_file2', type=str,   help='output file 2')\n",
        "    args = parser.parse_args()\n",
        "    main(\n",
        "        args.seed\n",
        "    ,   args.testsplit\n",
        "    ,   args.input_file\n",
        "    ,   args.output_file1\n",
        "    ,   args.output_file2\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE1gAoz1_RQ3",
        "outputId": "3b653c5d-18d2-46db-8be7-752427e4ef3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dataclean.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile dataclean.py\n",
        " # -*- coding: utf-8 -*-\n",
        "import abc\n",
        "import argparse\n",
        "from typing import List\n",
        "from telchurn.data_loader import DataLoader, DataLoaderImpl\n",
        "from telchurn.feature_processor import FeatureProcessor, FeatureProcessorImpl\n",
        "from telchurn.feature_ranker import FeatureRanker, FeatureRankerImpl\n",
        "from telchurn.feature_selector import FeatureSelector, FeatureSelectorImpl\n",
        "from telchurn.data_cleaner import DataCleaner, DataCleanerImpl\n",
        "import telchurn.util as util\n",
        "\n",
        "from telchurn.data_loader import DataLoader\n",
        "\n",
        "LOGGER = util.get_logger('dataclean')\n",
        "        \n",
        "def main(input_file: str, output_file: str, topk: int, seed: int, fields: List[str]):\n",
        "    data_loader = DataLoaderImpl()\n",
        "    feature_processor = FeatureProcessorImpl(seed)\n",
        "    feature_ranker = FeatureRankerImpl(seed)\n",
        "    feature_selector = FeatureSelectorImpl(feature_ranker)\n",
        "    data_cleaner = DataCleanerImpl(data_loader, feature_processor, feature_selector)\n",
        "    data_cleaner.clean(input_file, output_file, topk, fields)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--seed', type=int, help='number of features to keep', default=DataCleaner.DEFAULT_SEED)\n",
        "    parser.add_argument('--topk', type=int, help='number of features to keep', default=DataCleaner.DEFAULT_TOP_K_FEATURES)\n",
        "    parser.add_argument('--fields', type=str, help='comma separated list of columns to be kept', default=None)\n",
        "    parser.add_argument('input_file', type=str, help='input file name or url')\n",
        "    parser.add_argument('output_file', type=str, help='output file name')\n",
        "    args = parser.parse_args()\n",
        "    main(args.input_file, args.output_file, args.topk, args.seed, args.fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XTUNqk-_WU0",
        "outputId": "67b1f144-b6b8-4643-efb3-053f92d72a6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fieldnames.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile fieldnames.py\n",
        "# -*- coding: utf-8 -*-\n",
        "import abc\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from telchurn.data_loader import DataLoader, DataLoaderImpl\n",
        "import telchurn.util as util\n",
        "\n",
        "LOGGER = util.get_logger('fieldnames')\n",
        "\n",
        "def main(input_file: str, output_file: str) -> None:\n",
        "    data_loader = DataLoaderImpl()\n",
        "    LOGGER.info('starting data splitter')\n",
        "    df = data_loader.load_cleansed(input_file) # GAMBIARRA: load cleansed does not alter the input file\n",
        "    if output_file == \"-\":\n",
        "        print(*df.columns, sep=DataLoader.DELIMITER)\n",
        "    else:\n",
        "        with open(output_file, \"w\") as fh:\n",
        "            print(*df.columns, sep=DataLoader.DELIMITER, file=fh, end='')\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('input_file', type=str,   help='input file name')\n",
        "    parser.add_argument('output_file', type=str,   help='output file name')\n",
        "    args = parser.parse_args()\n",
        "    main(args.input_file, args.output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c76JHeaa_hBd",
        "outputId": "85134e07-6bbf-43f7-9caf-6ba7407a86b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./train.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./train.py\n",
        "# -*- coding: utf-8 -*-\n",
        "import abc\n",
        "import argparse\n",
        "from telchurn.data_loader import DataLoader, DataLoaderImpl\n",
        "from telchurn.data_splitter import DataSplitter, DataSplitterImpl\n",
        "from telchurn.feature_processor import FeatureProcessor, FeatureProcessorImpl\n",
        "from telchurn.feature_ranker import FeatureRanker, FeatureRankerImpl\n",
        "from telchurn.feature_selector import FeatureSelector, FeatureSelectorImpl\n",
        "from telchurn.pipeline_factory import PipelineFactory, PipelineFactoryImpl\n",
        "from telchurn.hyper_param_tunner import HyperParamTunner, HyperParamTunnerImpl\n",
        "from telchurn.model_repository import ModelRepository, ModelRepositoryImpl\n",
        "import telchurn.param_grids as param_grids\n",
        "from telchurn.trainer import Trainer, TrainerImpl\n",
        "import telchurn.util as util\n",
        "\n",
        "from telchurn.data_loader import DataLoader\n",
        "\n",
        "LOGGER = util.get_logger('train')\n",
        "        \n",
        "def main(input_file: str, seed: int, testsplit: float, kfolds: int, model_dir: str, quick: bool):\n",
        "    if quick:\n",
        "        LOGGER.warn('activating quick run mode')\n",
        "        param_grids.QUICK_RUN = True\n",
        "    data_loader = DataLoaderImpl()\n",
        "    pipeline_factory = PipelineFactoryImpl()\n",
        "    hp_tunner = HyperParamTunnerImpl(kfolds, seed)\n",
        "    repo = ModelRepositoryImpl(model_dir)\n",
        "    splitter = DataSplitterImpl(seed, testsplit)\n",
        "    trainer = TrainerImpl(data_loader, pipeline_factory, hp_tunner, repo)\n",
        "    trainer.train(input_file, splitter)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--seed',       type=int,   help='random seed',             default=DataSplitter.DEFAULT_RANDOM_STATE)\n",
        "    parser.add_argument('--testsplit',  type=float, help='test split percentage',   default=DataSplitter.DEFAULT_TEST_PCT_SIZE)\n",
        "    parser.add_argument('--kfolds',     type=int,   help='number of k folds',       default=HyperParamTunner.DEFAULT_K_FOLDS)\n",
        "    parser.add_argument('--quick',      action=\"store_true\", help='quick run', default=False)\n",
        "    parser.add_argument('input_file',   type=str,   help='input file name')\n",
        "    parser.add_argument('model_dir',    type=str,   help='models output directory')\n",
        "    args = parser.parse_args()\n",
        "    main(args.input_file, args.seed, args.testsplit, args.kfolds, args.model_dir, args.quick)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GsOG7jS_pqw",
        "outputId": "39417f3f-e9a3-4c01-8c7a-63744342d600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./ensembler.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./ensembler.py\n",
        "# -*- coding: utf-8 -*-\n",
        "import abc\n",
        "import argparse\n",
        "from telchurn.data_loader import DataLoader, DataLoaderImpl\n",
        "from telchurn.model_repository import ModelRepository, ModelRepositoryImpl\n",
        "from telchurn.ensembler import Ensembler, EnsemblerImpl\n",
        "from telchurn.data_splitter import DataSplitter\n",
        "from telchurn.hyper_param_tunner import HyperParamTunner\n",
        "from telchurn.data_splitter import DataSplitterImpl\n",
        "from telchurn.model_evaluator import ModelEvaluatorImpl\n",
        "import telchurn.util as util\n",
        "\n",
        "LOGGER = util.get_logger('ensembler')\n",
        "\n",
        "class App:\n",
        "\n",
        "    def __init__(self, data_loader: DataLoader, repo: ModelRepository, ensembler: Ensembler):\n",
        "        self.data_loader = data_loader\n",
        "        self.repo = repo\n",
        "        self.ensembler = ensembler\n",
        "    \n",
        "    def read_grids(self):\n",
        "        LOGGER.info('reading saved grids')\n",
        "        grids = []\n",
        "        for grid_name in self.repo.list_grids():\n",
        "            grid = self.repo.load_grid(grid_name)\n",
        "            grids.append(grid)\n",
        "        return grids\n",
        "        \n",
        "    def run(self, input_file_or_url: str, model_name: str) -> None:\n",
        "        LOGGER.info('starting ensembler')\n",
        "        grids = self.read_grids()\n",
        "        churn_df = self.data_loader.load_cleansed(input_file_or_url)\n",
        "        util.report_df(LOGGER, churn_df)\n",
        "        voting_classifier = self.ensembler.ensemble_models(grids, churn_df)\n",
        "        self.repo.save_final_model(voting_classifier, model_name)\n",
        "        \n",
        "        \n",
        "def main(input_file: str, seed: int, testsplit: float, kfolds: int, model_dir: str, model_name: str):\n",
        "    data_loader = DataLoaderImpl()\n",
        "    repo = ModelRepositoryImpl(model_dir)\n",
        "    evaluator = ModelEvaluatorImpl()\n",
        "    splitter = DataSplitterImpl(seed, testsplit)\n",
        "    ensembler = EnsemblerImpl(splitter, evaluator)\n",
        "    app = App(data_loader, repo, ensembler)\n",
        "    app.run(input_file, model_name)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--seed',       type=int,   help='random seed',             default=DataSplitter.DEFAULT_RANDOM_STATE)\n",
        "    parser.add_argument('--testsplit',  type=float, help='test split percentage',   default=DataSplitter.DEFAULT_TEST_PCT_SIZE)\n",
        "    parser.add_argument('--kfolds',     type=int,   help='number of k folds',       default=HyperParamTunner.DEFAULT_K_FOLDS)\n",
        "    parser.add_argument('input_file',   type=str,   help='input file name')\n",
        "    parser.add_argument('model_dir',    type=str,   help='models output directory')\n",
        "    parser.add_argument('model_name',   type=str,   help='final model name')\n",
        "    args = parser.parse_args()\n",
        "    main(args.input_file, args.seed, args.testsplit, args.kfolds, args.model_dir, args.model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91SNy2Oi_uBP",
        "outputId": "4b7ba449-743f-4530-8fa1-22375a5ffbbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing ./classify.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ./classify.py\n",
        " # -*- coding: utf-8 -*-\n",
        "import argparse\n",
        "import os.path\n",
        "from telchurn.model_repository import ModelRepositoryImpl\n",
        "from telchurn.data_loader import DataLoaderImpl\n",
        "from telchurn.model_repository import ModelRepositoryImpl\n",
        "from telchurn.model_evaluator import ModelEvaluatorImpl\n",
        "import telchurn.util as util\n",
        "\n",
        "LOGGER = util.get_logger('classify')\n",
        "        \n",
        "def main(model_path: str, input_file: str) -> None:\n",
        "    LOGGER.info('starting classifier')\n",
        "    model_dir = os.path.dirname(model_path)\n",
        "    model_file = os.path.basename(model_path)\n",
        "    repo = ModelRepositoryImpl(model_dir)\n",
        "    loader = DataLoaderImpl()\n",
        "    evaluator = ModelEvaluatorImpl()\n",
        "    estimator = repo.load_final_model(model_file)\n",
        "    churn_df = loader.load_cleansed(input_file)    \n",
        "    target = churn_df.columns[-1]\n",
        "    all_but_target = churn_df.columns.difference([target])\n",
        "    X_df = churn_df[all_but_target]\n",
        "    y_df = churn_df[target]\n",
        "    evaluator.report_results(estimator, X_df, y_df)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('model_file',   type=str,   help='saved model file')\n",
        "    parser.add_argument('input_file',   type=str,   help='input file name')\n",
        "    args = parser.parse_args()\n",
        "    main(args.model_file, args.input_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldGvy2L8DjJa"
      },
      "source": [
        "## Instalação das Dependências"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHo0qL78_1CV",
        "outputId": "eef23859-1a71-497b-efd5-29d525c2600e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Collecting shutup\n",
            "  Downloading shutup-0.2.0-py3-none-any.whl (1.5 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (1.15.0)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (from imblearn) (0.8.1)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from mlxtend) (57.4.0)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.7.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.5.1->mlxtend) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.5.1->mlxtend) (4.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->mlxtend) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->mlxtend) (3.1.0)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=9cb9ee25ade62a06fa7ad44eb2e9da57aeebd30eb21d30b05ddf4668733e27a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/56/cc/4a8bf86613aafd5b7f1b310477667c1fca5c51c3ae4124a003\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn, shutup\n",
            "Successfully installed shutup-0.2.0 sklearn-0.0.post1\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas imblearn mlxtend shutup six sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ble37dUV1hcE"
      },
      "source": [
        "## Baixando Arquivo de Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oj0VaIc1lC1",
        "outputId": "daf36676-0dfb-4e23-d170-f53a88f6dd3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-15 16:48:04--  https://raw.githubusercontent.com/itaborai83/ecd221-SI-trabalho/main/DATA/telco-churn.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 970457 (948K) [text/plain]\n",
            "Saving to: ‘telco-churn.csv’\n",
            "\n",
            "telco-churn.csv     100%[===================>] 947.71K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-11-15 16:48:04 (25.0 MB/s) - ‘telco-churn.csv’ saved [970457/970457]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O telco-churn.csv https://raw.githubusercontent.com/itaborai83/ecd221-SI-trabalho/main/DATA/telco-churn.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXldDGTeDu6U"
      },
      "source": [
        "## Configuração das variáveis de ambiente utilizadas como parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fweIfDpLACFd",
        "outputId": "fab4781e-ee53-42a3-c148-f8d575ff3c69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: SEED=42\n",
            "env: HOLDOUT_SPLIT_PCT=0.1\n",
            "env: TEST_SPLIT_PCT=0.3\n",
            "env: FEATURE_COUNT=14\n",
            "env: KFOLDS=10\n",
            "env: MODELS_DIR=./MODELS\n",
            "env: INPUT_FILE=./telco-churn.csv\n",
            "env: TRAIN_TEST_FILE=./DATA/telco-churn-train-test.csv\n",
            "env: HOLD_OUT_FILE=./DATA/telco-churn-holdout.csv\n",
            "env: TRAIN_TEST_FILE_CLEAN=./DATA/telco-churn-train-test-clean.csv\n",
            "env: HOLD_OUT_FILE_CLEAN=./DATA/telco-churn-holdout-clean.csv\n",
            "env: FIELDNAMES=./DATA/fieldnames.txt\n",
            "env: PYTHONWARNINGS=ignore::UserWarning,ignore::FutureWarning\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%env SEED=42\n",
        "%env HOLDOUT_SPLIT_PCT=0.1\n",
        "%env TEST_SPLIT_PCT=0.3\n",
        "%env FEATURE_COUNT=14\n",
        "%env KFOLDS=10\n",
        "%env MODELS_DIR=./MODELS\n",
        "%env INPUT_FILE=./telco-churn.csv\n",
        "%env TRAIN_TEST_FILE=./DATA/telco-churn-train-test.csv\n",
        "%env HOLD_OUT_FILE=./DATA/telco-churn-holdout.csv\n",
        "%env TRAIN_TEST_FILE_CLEAN=./DATA/telco-churn-train-test-clean.csv\n",
        "%env HOLD_OUT_FILE_CLEAN=./DATA/telco-churn-holdout-clean.csv\n",
        "%env FIELDNAMES=./DATA/fieldnames.txt\n",
        "%env PYTHONWARNINGS=ignore::UserWarning,ignore::FutureWarning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8-_QahBD35m"
      },
      "source": [
        "## Geração do Arquivo de Treino/Test e Arquivo de Hold Out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp2O5h-wAM-G",
        "outputId": "34541ded-8072-4775-ecb9-29859ebdabfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO - splitter.py:main:14 - starting data splitter\n",
            "INFO - data_loader.py:load_cleansed:62 - loading cleansed dataframe from ./telco-churn.csv\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7043 entries, 0 to 7042\n",
            "Data columns (total 21 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   customerID        7043 non-null   object \n",
            " 1   gender            7043 non-null   object \n",
            " 2   SeniorCitizen     7043 non-null   int64  \n",
            " 3   Partner           7043 non-null   object \n",
            " 4   Dependents        7043 non-null   object \n",
            " 5   tenure            7043 non-null   int64  \n",
            " 6   PhoneService      7043 non-null   object \n",
            " 7   MultipleLines     7043 non-null   object \n",
            " 8   InternetService   7043 non-null   object \n",
            " 9   OnlineSecurity    7043 non-null   object \n",
            " 10  OnlineBackup      7043 non-null   object \n",
            " 11  DeviceProtection  7043 non-null   object \n",
            " 12  TechSupport       7043 non-null   object \n",
            " 13  StreamingTV       7043 non-null   object \n",
            " 14  StreamingMovies   7043 non-null   object \n",
            " 15  Contract          7043 non-null   object \n",
            " 16  PaperlessBilling  7043 non-null   object \n",
            " 17  PaymentMethod     7043 non-null   object \n",
            " 18  MonthlyCharges    7043 non-null   float64\n",
            " 19  TotalCharges      7043 non-null   object \n",
            " 20  Churn             7043 non-null   object \n",
            "dtypes: float64(1), int64(2), object(18)\n",
            "memory usage: 1.1+ MB\n",
            "\n",
            "INFO - splitter.py:main:16 - input file: ./telco-churn.csv\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7043 entries, 0 to 7042\n",
            "Data columns (total 21 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   customerID        7043 non-null   object \n",
            " 1   gender            7043 non-null   object \n",
            " 2   SeniorCitizen     7043 non-null   int64  \n",
            " 3   Partner           7043 non-null   object \n",
            " 4   Dependents        7043 non-null   object \n",
            " 5   tenure            7043 non-null   int64  \n",
            " 6   PhoneService      7043 non-null   object \n",
            " 7   MultipleLines     7043 non-null   object \n",
            " 8   InternetService   7043 non-null   object \n",
            " 9   OnlineSecurity    7043 non-null   object \n",
            " 10  OnlineBackup      7043 non-null   object \n",
            " 11  DeviceProtection  7043 non-null   object \n",
            " 12  TechSupport       7043 non-null   object \n",
            " 13  StreamingTV       7043 non-null   object \n",
            " 14  StreamingMovies   7043 non-null   object \n",
            " 15  Contract          7043 non-null   object \n",
            " 16  PaperlessBilling  7043 non-null   object \n",
            " 17  PaymentMethod     7043 non-null   object \n",
            " 18  MonthlyCharges    7043 non-null   float64\n",
            " 19  TotalCharges      7043 non-null   object \n",
            " 20  Churn             7043 non-null   object \n",
            "dtypes: float64(1), int64(2), object(18)\n",
            "memory usage: 1.1+ MB\n",
            "\n",
            "INFO - splitter.py:main:19 - using column \"Churn\" as target variable\n",
            "INFO - data_splitter.py:split:33 - splitting data set into train and test sets\n",
            "INFO - splitter.py:main:30 - output file 1: ./DATA/telco-churn-train-test.csv\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6338 entries, 0 to 6337\n",
            "Data columns (total 21 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   customerID        6338 non-null   object\n",
            " 1   gender            6338 non-null   object\n",
            " 2   SeniorCitizen     6338 non-null   object\n",
            " 3   Partner           6338 non-null   object\n",
            " 4   Dependents        6338 non-null   object\n",
            " 5   tenure            6338 non-null   object\n",
            " 6   PhoneService      6338 non-null   object\n",
            " 7   MultipleLines     6338 non-null   object\n",
            " 8   InternetService   6338 non-null   object\n",
            " 9   OnlineSecurity    6338 non-null   object\n",
            " 10  OnlineBackup      6338 non-null   object\n",
            " 11  DeviceProtection  6338 non-null   object\n",
            " 12  TechSupport       6338 non-null   object\n",
            " 13  StreamingTV       6338 non-null   object\n",
            " 14  StreamingMovies   6338 non-null   object\n",
            " 15  Contract          6338 non-null   object\n",
            " 16  PaperlessBilling  6338 non-null   object\n",
            " 17  PaymentMethod     6338 non-null   object\n",
            " 18  MonthlyCharges    6338 non-null   object\n",
            " 19  TotalCharges      6338 non-null   object\n",
            " 20  Churn             6338 non-null   object\n",
            "dtypes: object(21)\n",
            "memory usage: 1.0+ MB\n",
            "\n",
            "INFO - splitter.py:main:32 - output file 2: ./DATA/telco-churn-holdout.csv\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 705 entries, 0 to 704\n",
            "Data columns (total 21 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   customerID        705 non-null    object\n",
            " 1   gender            705 non-null    object\n",
            " 2   SeniorCitizen     705 non-null    object\n",
            " 3   Partner           705 non-null    object\n",
            " 4   Dependents        705 non-null    object\n",
            " 5   tenure            705 non-null    object\n",
            " 6   PhoneService      705 non-null    object\n",
            " 7   MultipleLines     705 non-null    object\n",
            " 8   InternetService   705 non-null    object\n",
            " 9   OnlineSecurity    705 non-null    object\n",
            " 10  OnlineBackup      705 non-null    object\n",
            " 11  DeviceProtection  705 non-null    object\n",
            " 12  TechSupport       705 non-null    object\n",
            " 13  StreamingTV       705 non-null    object\n",
            " 14  StreamingMovies   705 non-null    object\n",
            " 15  Contract          705 non-null    object\n",
            " 16  PaperlessBilling  705 non-null    object\n",
            " 17  PaymentMethod     705 non-null    object\n",
            " 18  MonthlyCharges    705 non-null    object\n",
            " 19  TotalCharges      705 non-null    object\n",
            " 20  Churn             705 non-null    object\n",
            "dtypes: object(21)\n",
            "memory usage: 115.8+ KB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! python splitter.py --seed $SEED --testsplit $HOLDOUT_SPLIT_PCT $INPUT_FILE $TRAIN_TEST_FILE $HOLD_OUT_FILE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqUXak9oEAI4"
      },
      "source": [
        "## Limpeza, Featuring Engineering e Feature Selection Determinística do Arquivo de Treino/Teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-hAqeH0BWAz",
        "outputId": "7a362b0e-5268-4d22-dd1e-753aad9d8d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO - data_cleaner.py:clean:35 - starting data cleaner\n",
            "INFO - data_loader.py:load:51 - loading dataframe from ./DATA/telco-churn-train-test.csv\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6338 entries, 0 to 6337\n",
            "Data columns (total 21 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   customer_id        6338 non-null   object \n",
            " 1   gender             6338 non-null   object \n",
            " 2   senior_citizen     6338 non-null   int64  \n",
            " 3   partner            6338 non-null   object \n",
            " 4   dependents         6338 non-null   object \n",
            " 5   tenure             6338 non-null   int64  \n",
            " 6   phone_service      6338 non-null   object \n",
            " 7   multiple_lines     6338 non-null   object \n",
            " 8   internet_service   6338 non-null   object \n",
            " 9   online_security    6338 non-null   object \n",
            " 10  online_backup      6338 non-null   object \n",
            " 11  device_protection  6338 non-null   object \n",
            " 12  tech_support       6338 non-null   object \n",
            " 13  streaming_tv       6338 non-null   object \n",
            " 14  streaming_movies   6338 non-null   object \n",
            " 15  contract           6338 non-null   object \n",
            " 16  paperless_billing  6338 non-null   object \n",
            " 17  payment_method     6338 non-null   object \n",
            " 18  monthly_charges    6338 non-null   float64\n",
            " 19  total_charges      6338 non-null   object \n",
            " 20  churn              6338 non-null   object \n",
            "dtypes: float64(1), int64(2), object(18)\n",
            "memory usage: 1.0+ MB\n",
            "\n",
            "INFO - feature_processor.py:handle_categorical_features:62 - handling categorical features\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6338 entries, 0 to 6337\n",
            "Data columns (total 25 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   senior_citizen     6338 non-null   int64  \n",
            " 1   partner            6338 non-null   int64  \n",
            " 2   dependents         6338 non-null   int64  \n",
            " 3   phone_service      6338 non-null   int64  \n",
            " 4   paperless_billing  6338 non-null   int64  \n",
            " 5   multiple_lines     6338 non-null   uint8  \n",
            " 6   dsl                6338 non-null   uint8  \n",
            " 7   fiber_optic        6338 non-null   uint8  \n",
            " 8   online_security    6338 non-null   uint8  \n",
            " 9   online_backup      6338 non-null   uint8  \n",
            " 10  device_protection  6338 non-null   uint8  \n",
            " 11  tech_support       6338 non-null   uint8  \n",
            " 12  streaming_tv       6338 non-null   uint8  \n",
            " 13  streaming_movies   6338 non-null   uint8  \n",
            " 14  monthly_contract   6338 non-null   uint8  \n",
            " 15  one_year_contract  6338 non-null   uint8  \n",
            " 16  two_year_contract  6338 non-null   uint8  \n",
            " 17  bank_transfer      6338 non-null   uint8  \n",
            " 18  credit_card        6338 non-null   uint8  \n",
            " 19  electronic_check   6338 non-null   uint8  \n",
            " 20  mailed_check       6338 non-null   uint8  \n",
            " 21  tenure             6338 non-null   int64  \n",
            " 22  monthly_charges    6338 non-null   float64\n",
            " 23  total_charges      6338 non-null   float64\n",
            " 24  churn              6338 non-null   int64  \n",
            "dtypes: float64(2), int64(7), uint8(16)\n",
            "memory usage: 544.8 KB\n",
            "\n",
            "INFO - feature_processor.py:engineer_features:129 - engineering new features\n",
            "INFO - feature_processor.py:engineer_features:144 - creating client factor\n",
            "INFO - feature_processor.py:engineer_features:159 - creating internet factor\n",
            "INFO - feature_processor.py:engineer_features:178 - creating financial factor\n",
            "INFO - feature_processor.py:engineer_features:193 - combining factors into one\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6338 entries, 0 to 6337\n",
            "Data columns (total 29 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   senior_citizen     6338 non-null   int64  \n",
            " 1   partner            6338 non-null   int64  \n",
            " 2   dependents         6338 non-null   int64  \n",
            " 3   phone_service      6338 non-null   int64  \n",
            " 4   paperless_billing  6338 non-null   int64  \n",
            " 5   multiple_lines     6338 non-null   uint8  \n",
            " 6   dsl                6338 non-null   uint8  \n",
            " 7   fiber_optic        6338 non-null   uint8  \n",
            " 8   online_security    6338 non-null   uint8  \n",
            " 9   online_backup      6338 non-null   uint8  \n",
            " 10  device_protection  6338 non-null   uint8  \n",
            " 11  tech_support       6338 non-null   uint8  \n",
            " 12  streaming_tv       6338 non-null   uint8  \n",
            " 13  streaming_movies   6338 non-null   uint8  \n",
            " 14  monthly_contract   6338 non-null   uint8  \n",
            " 15  one_year_contract  6338 non-null   uint8  \n",
            " 16  two_year_contract  6338 non-null   uint8  \n",
            " 17  bank_transfer      6338 non-null   uint8  \n",
            " 18  credit_card        6338 non-null   uint8  \n",
            " 19  electronic_check   6338 non-null   uint8  \n",
            " 20  mailed_check       6338 non-null   uint8  \n",
            " 21  tenure             6338 non-null   int64  \n",
            " 22  monthly_charges    6338 non-null   float64\n",
            " 23  total_charges      6338 non-null   float64\n",
            " 24  client_factor      6338 non-null   float64\n",
            " 25  internet_factor    6338 non-null   float64\n",
            " 26  financial_factor   6338 non-null   float64\n",
            " 27  multi_factor       6338 non-null   float64\n",
            " 28  churn              6338 non-null   int64  \n",
            "dtypes: float64(6), int64(7), uint8(16)\n",
            "memory usage: 742.9 KB\n",
            "\n",
            "INFO - feature_selector.py:select_features:23 - selecting top 14 features\n",
            "INFO - feature_ranker.py:rank_features:28 - ranking features\n",
            "INFO - feature_ranker.py:rank_features:39 - fitting random forest classifier\n",
            "INFO - feature_ranker.py:rank_features:50 - Ranked Features\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> total_charges: 0.17308121303557916\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> monthly_charges: 0.16984018771109619\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> tenure: 0.1549072513596169\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> monthly_contract: 0.09145421980582108\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> two_year_contract: 0.04819340790454489\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> fiber_optic: 0.03918627805409081\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> electronic_check: 0.03511825049328907\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> paperless_billing: 0.026293331334201524\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> partner: 0.022065015698160495\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> online_security: 0.02138572444595611\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> tech_support: 0.020330190599483274\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> online_backup: 0.019951422005094813\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> dependents: 0.019486152999343902\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> one_year_contract: 0.01850062768162159\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> senior_citizen: 0.018255193513160645\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> streaming_movies: 0.017984619350901274\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> multiple_lines: 0.017811454782872643\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> device_protection: 0.017788398507285638\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> streaming_tv: 0.016055961111537564\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> bank_transfer: 0.011761410471906642\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> credit_card: 0.011431147991516793\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> dsl: 0.011190592587774217\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> mailed_check: 0.010822470571254334\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> phone_service: 0.007105477983890425\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> client_factor: 0.0\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> multi_factor: 0.0\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> internet_factor: 0.0\n",
            "INFO - feature_ranker.py:rank_features:52 - \t-> financial_factor: 0.0\n",
            "INFO - data_cleaner.py:save_cleansed:65 - saving cleansed dataframe to ./DATA/telco-churn-train-test-clean.csv\n"
          ]
        }
      ],
      "source": [
        "!python dataclean.py --seed $SEED --topk $FEATURE_COUNT $TRAIN_TEST_FILE $TRAIN_TEST_FILE_CLEAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXTlZcP-ENyv"
      },
      "source": [
        "# Recuperação dos Features Selecionados para uso no processamento do arquivo de Hold Out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvzZ55LYB_SX",
        "outputId": "8b9090bc-8e61-44fa-b251-ebde5dc6207f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO - fieldnames.py:main:12 - starting data splitter\n",
            "INFO - data_loader.py:load_cleansed:62 - loading cleansed dataframe from ./DATA/telco-churn-train-test-clean.csv\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6338 entries, 0 to 6337\n",
            "Data columns (total 15 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   total_charges      6338 non-null   float64\n",
            " 1   monthly_charges    6338 non-null   float64\n",
            " 2   tenure             6338 non-null   int64  \n",
            " 3   monthly_contract   6338 non-null   int64  \n",
            " 4   two_year_contract  6338 non-null   int64  \n",
            " 5   fiber_optic        6338 non-null   int64  \n",
            " 6   electronic_check   6338 non-null   int64  \n",
            " 7   paperless_billing  6338 non-null   int64  \n",
            " 8   partner            6338 non-null   int64  \n",
            " 9   online_security    6338 non-null   int64  \n",
            " 10  tech_support       6338 non-null   int64  \n",
            " 11  online_backup      6338 non-null   int64  \n",
            " 12  dependents         6338 non-null   int64  \n",
            " 13  one_year_contract  6338 non-null   int64  \n",
            " 14  churn              6338 non-null   int64  \n",
            "dtypes: float64(2), int64(13)\n",
            "memory usage: 742.9 KB\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python fieldnames.py $TRAIN_TEST_FILE_CLEAN $FIELDNAMES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdSLrWgUEWzs"
      },
      "source": [
        "## Limpeza, Featuring Engineering e Feature Selection Determinística do Arquivo de Hold Out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5ooCH9eCSS5",
        "outputId": "eab0cd30-8304-4416-c365-7d1ec0065c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO - data_cleaner.py:clean:35 - starting data cleaner\n",
            "INFO - data_loader.py:load:51 - loading dataframe from ./DATA/telco-churn-holdout.csv\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 705 entries, 0 to 704\n",
            "Data columns (total 21 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   customer_id        705 non-null    object \n",
            " 1   gender             705 non-null    object \n",
            " 2   senior_citizen     705 non-null    int64  \n",
            " 3   partner            705 non-null    object \n",
            " 4   dependents         705 non-null    object \n",
            " 5   tenure             705 non-null    int64  \n",
            " 6   phone_service      705 non-null    object \n",
            " 7   multiple_lines     705 non-null    object \n",
            " 8   internet_service   705 non-null    object \n",
            " 9   online_security    705 non-null    object \n",
            " 10  online_backup      705 non-null    object \n",
            " 11  device_protection  705 non-null    object \n",
            " 12  tech_support       705 non-null    object \n",
            " 13  streaming_tv       705 non-null    object \n",
            " 14  streaming_movies   705 non-null    object \n",
            " 15  contract           705 non-null    object \n",
            " 16  paperless_billing  705 non-null    object \n",
            " 17  payment_method     705 non-null    object \n",
            " 18  monthly_charges    705 non-null    float64\n",
            " 19  total_charges      705 non-null    object \n",
            " 20  churn              705 non-null    object \n",
            "dtypes: float64(1), int64(2), object(18)\n",
            "memory usage: 115.8+ KB\n",
            "\n",
            "INFO - feature_processor.py:handle_categorical_features:62 - handling categorical features\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 705 entries, 0 to 704\n",
            "Data columns (total 25 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   senior_citizen     705 non-null    int64  \n",
            " 1   partner            705 non-null    int64  \n",
            " 2   dependents         705 non-null    int64  \n",
            " 3   phone_service      705 non-null    int64  \n",
            " 4   paperless_billing  705 non-null    int64  \n",
            " 5   multiple_lines     705 non-null    uint8  \n",
            " 6   dsl                705 non-null    uint8  \n",
            " 7   fiber_optic        705 non-null    uint8  \n",
            " 8   online_security    705 non-null    uint8  \n",
            " 9   online_backup      705 non-null    uint8  \n",
            " 10  device_protection  705 non-null    uint8  \n",
            " 11  tech_support       705 non-null    uint8  \n",
            " 12  streaming_tv       705 non-null    uint8  \n",
            " 13  streaming_movies   705 non-null    uint8  \n",
            " 14  monthly_contract   705 non-null    uint8  \n",
            " 15  one_year_contract  705 non-null    uint8  \n",
            " 16  two_year_contract  705 non-null    uint8  \n",
            " 17  bank_transfer      705 non-null    uint8  \n",
            " 18  credit_card        705 non-null    uint8  \n",
            " 19  electronic_check   705 non-null    uint8  \n",
            " 20  mailed_check       705 non-null    uint8  \n",
            " 21  tenure             705 non-null    int64  \n",
            " 22  monthly_charges    705 non-null    float64\n",
            " 23  total_charges      705 non-null    float64\n",
            " 24  churn              705 non-null    int64  \n",
            "dtypes: float64(2), int64(7), uint8(16)\n",
            "memory usage: 60.7 KB\n",
            "\n",
            "INFO - feature_processor.py:engineer_features:129 - engineering new features\n",
            "INFO - feature_processor.py:engineer_features:144 - creating client factor\n",
            "INFO - feature_processor.py:engineer_features:159 - creating internet factor\n",
            "INFO - feature_processor.py:engineer_features:178 - creating financial factor\n",
            "INFO - feature_processor.py:engineer_features:193 - combining factors into one\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 705 entries, 0 to 704\n",
            "Data columns (total 29 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   senior_citizen     705 non-null    int64  \n",
            " 1   partner            705 non-null    int64  \n",
            " 2   dependents         705 non-null    int64  \n",
            " 3   phone_service      705 non-null    int64  \n",
            " 4   paperless_billing  705 non-null    int64  \n",
            " 5   multiple_lines     705 non-null    uint8  \n",
            " 6   dsl                705 non-null    uint8  \n",
            " 7   fiber_optic        705 non-null    uint8  \n",
            " 8   online_security    705 non-null    uint8  \n",
            " 9   online_backup      705 non-null    uint8  \n",
            " 10  device_protection  705 non-null    uint8  \n",
            " 11  tech_support       705 non-null    uint8  \n",
            " 12  streaming_tv       705 non-null    uint8  \n",
            " 13  streaming_movies   705 non-null    uint8  \n",
            " 14  monthly_contract   705 non-null    uint8  \n",
            " 15  one_year_contract  705 non-null    uint8  \n",
            " 16  two_year_contract  705 non-null    uint8  \n",
            " 17  bank_transfer      705 non-null    uint8  \n",
            " 18  credit_card        705 non-null    uint8  \n",
            " 19  electronic_check   705 non-null    uint8  \n",
            " 20  mailed_check       705 non-null    uint8  \n",
            " 21  tenure             705 non-null    int64  \n",
            " 22  monthly_charges    705 non-null    float64\n",
            " 23  total_charges      705 non-null    float64\n",
            " 24  client_factor      705 non-null    float64\n",
            " 25  internet_factor    705 non-null    float64\n",
            " 26  financial_factor   705 non-null    float64\n",
            " 27  multi_factor       705 non-null    float64\n",
            " 28  churn              705 non-null    int64  \n",
            "dtypes: float64(6), int64(7), uint8(16)\n",
            "memory usage: 82.7 KB\n",
            "\n",
            "INFO - data_cleaner.py:save_cleansed:65 - saving cleansed dataframe to ./DATA/telco-churn-holdout-clean.csv\n"
          ]
        }
      ],
      "source": [
        "! python dataclean.py  --seed $SEED --topk $FEATURE_COUNT --fields=$(cat $FIELDNAMES) $HOLD_OUT_FILE $HOLD_OUT_FILE_CLEAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2yAxfNWEf9e"
      },
      "source": [
        "# Treino e ajuste dos hiper-parâmetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL8XcO_SC3N4",
        "outputId": "a6b6b6c9-0a21-4e0d-f07b-da9f5c6b8f9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO - trainer.py:train:41 - starting telco churn model training\n",
            "INFO - data_loader.py:load_cleansed:62 - loading cleansed dataframe from ./DATA/telco-churn-train-test-clean.csv\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6338 entries, 0 to 6337\n",
            "Data columns (total 15 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   total_charges      6338 non-null   float64\n",
            " 1   monthly_charges    6338 non-null   float64\n",
            " 2   tenure             6338 non-null   int64  \n",
            " 3   monthly_contract   6338 non-null   int64  \n",
            " 4   two_year_contract  6338 non-null   int64  \n",
            " 5   fiber_optic        6338 non-null   int64  \n",
            " 6   electronic_check   6338 non-null   int64  \n",
            " 7   paperless_billing  6338 non-null   int64  \n",
            " 8   partner            6338 non-null   int64  \n",
            " 9   online_security    6338 non-null   int64  \n",
            " 10  tech_support       6338 non-null   int64  \n",
            " 11  online_backup      6338 non-null   int64  \n",
            " 12  dependents         6338 non-null   int64  \n",
            " 13  one_year_contract  6338 non-null   int64  \n",
            " 14  churn              6338 non-null   int64  \n",
            "dtypes: float64(2), int64(13)\n",
            "memory usage: 742.9 KB\n",
            "\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6338 entries, 0 to 6337\n",
            "Data columns (total 15 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   total_charges      6338 non-null   float64\n",
            " 1   monthly_charges    6338 non-null   float64\n",
            " 2   tenure             6338 non-null   int64  \n",
            " 3   monthly_contract   6338 non-null   int64  \n",
            " 4   two_year_contract  6338 non-null   int64  \n",
            " 5   fiber_optic        6338 non-null   int64  \n",
            " 6   electronic_check   6338 non-null   int64  \n",
            " 7   paperless_billing  6338 non-null   int64  \n",
            " 8   partner            6338 non-null   int64  \n",
            " 9   online_security    6338 non-null   int64  \n",
            " 10  tech_support       6338 non-null   int64  \n",
            " 11  online_backup      6338 non-null   int64  \n",
            " 12  dependents         6338 non-null   int64  \n",
            " 13  one_year_contract  6338 non-null   int64  \n",
            " 14  churn              6338 non-null   int64  \n",
            "dtypes: float64(2), int64(13)\n",
            "memory usage: 742.9 KB\n",
            "\n",
            "INFO - data_splitter.py:split:33 - splitting data set into train and test sets\n",
            "INFO - param_grids.py:get_logreg_grid:63 - creating parameter grids for logistic Regression - LOGREG\n",
            "INFO - param_grids.py:get_knn_grid:90 - creating parameter grids for K Nearest Neighbors - KNN\n",
            "INFO - param_grids.py:get_nb_grid:110 - creating parameter grids for Naive Bayes - nb\n",
            "INFO - param_grids.py:get_dt_grid:125 - creating parameter grids for Decision Tree - DT\n",
            "INFO - param_grids.py:get_ada_grid:166 - creating parameter grids for AdaBoost classifier - ADA\n",
            "INFO - param_grids.py:get_gb_grid:182 - creating parameter grids for Gradient Boosting classifier - GB\n",
            "INFO - param_grids.py:get_rf_grid:201 - creating parameter grids for Random Forest classifier - RF\n",
            "INFO - hyper_param_tunner.py:tune:42 - tunning pipeline LOGREG using scoring metric balanced_accuracy\n",
            "INFO - hyper_param_tunner.py:tune:55 - Best balanced_accuracy: 0.770271896543603\n",
            "INFO - hyper_param_tunner.py:tune:56 - Best estimator: -> \n",
            "Pipeline(steps=[('feature_scaling',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('scaler',\n",
            "                                                                   'passthrough')]),\n",
            "                                                  ['tenure', 'monthly_charges',\n",
            "                                                   'total_charges'])])),\n",
            "                ('reduce_dim', PCA(n_components=5)),\n",
            "                ('classifier',\n",
            "                 LogisticRegression(class_weight='balanced', n_jobs=-1,\n",
            "                                    penalty='none'))])\n",
            "INFO - model_repository.py:save_grid:57 - saving grid search results to ./MODELS/grid_LOGREG.pkl\n",
            "INFO - hyper_param_tunner.py:tune:42 - tunning pipeline KNN using scoring metric balanced_accuracy\n",
            "INFO - hyper_param_tunner.py:tune:55 - Best balanced_accuracy: 0.7092982698906585\n",
            "INFO - hyper_param_tunner.py:tune:56 - Best estimator: -> \n",
            "Pipeline(steps=[('feature_scaling',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  ['tenure', 'monthly_charges',\n",
            "                                                   'total_charges'])])),\n",
            "                ('reduce_dim', 'passthrough'),\n",
            "                ('classifier',\n",
            "                 KNeighborsClassifier(algorithm='kd_tree', n_jobs=-1,\n",
            "                                      n_neighbors=15, p=1.0))])\n",
            "INFO - model_repository.py:save_grid:57 - saving grid search results to ./MODELS/grid_KNN.pkl\n",
            "INFO - hyper_param_tunner.py:tune:42 - tunning pipeline NB using scoring metric balanced_accuracy\n",
            "INFO - hyper_param_tunner.py:tune:55 - Best balanced_accuracy: 0.7538856787615237\n",
            "INFO - hyper_param_tunner.py:tune:56 - Best estimator: -> \n",
            "Pipeline(steps=[('feature_scaling',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  ['tenure', 'monthly_charges',\n",
            "                                                   'total_charges'])])),\n",
            "                ('reduce_dim', 'passthrough'),\n",
            "                ('classifier', GaussianNB(var_smoothing=0.1))])\n",
            "INFO - model_repository.py:save_grid:57 - saving grid search results to ./MODELS/grid_NB.pkl\n",
            "INFO - hyper_param_tunner.py:tune:42 - tunning pipeline DT using scoring metric balanced_accuracy\n",
            "INFO - hyper_param_tunner.py:tune:55 - Best balanced_accuracy: 0.7412079455095989\n",
            "INFO - hyper_param_tunner.py:tune:56 - Best estimator: -> \n",
            "Pipeline(steps=[('feature_scaling',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  ['tenure', 'monthly_charges',\n",
            "                                                   'total_charges'])])),\n",
            "                ('reduce_dim', PCA(n_components=3)),\n",
            "                ('classifier',\n",
            "                 DecisionTreeClassifier(class_weight='balanced',\n",
            "                                        criterion='entropy', max_depth=10,\n",
            "                                        splitter='random'))])\n",
            "INFO - model_repository.py:save_grid:57 - saving grid search results to ./MODELS/grid_DT.pkl\n",
            "INFO - hyper_param_tunner.py:tune:42 - tunning pipeline ADA using scoring metric balanced_accuracy\n",
            "INFO - hyper_param_tunner.py:tune:55 - Best balanced_accuracy: 0.721219843066365\n",
            "INFO - hyper_param_tunner.py:tune:56 - Best estimator: -> \n",
            "Pipeline(steps=[('feature_scaling',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  ['tenure', 'monthly_charges',\n",
            "                                                   'total_charges'])])),\n",
            "                ('reduce_dim', PCA(n_components=5)),\n",
            "                ('classifier', AdaBoostClassifier(n_estimators=100))])\n",
            "INFO - model_repository.py:save_grid:57 - saving grid search results to ./MODELS/grid_ADA.pkl\n",
            "INFO - hyper_param_tunner.py:tune:42 - tunning pipeline GB using scoring metric balanced_accuracy\n",
            "INFO - hyper_param_tunner.py:tune:55 - Best balanced_accuracy: 0.7145103027135886\n",
            "INFO - hyper_param_tunner.py:tune:56 - Best estimator: -> \n",
            "Pipeline(steps=[('feature_scaling',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  ['tenure', 'monthly_charges',\n",
            "                                                   'total_charges'])])),\n",
            "                ('reduce_dim', PCA(n_components=5)),\n",
            "                ('classifier',\n",
            "                 GradientBoostingClassifier(loss='exponential',\n",
            "                                            n_estimators=150))])\n",
            "INFO - model_repository.py:save_grid:57 - saving grid search results to ./MODELS/grid_GB.pkl\n",
            "INFO - hyper_param_tunner.py:tune:42 - tunning pipeline RF using scoring metric balanced_accuracy\n",
            "INFO - hyper_param_tunner.py:tune:55 - Best balanced_accuracy: 0.7635120019836666\n",
            "INFO - hyper_param_tunner.py:tune:56 - Best estimator: -> \n",
            "Pipeline(steps=[('feature_scaling',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('scaler',\n",
            "                                                                   'passthrough')]),\n",
            "                                                  ['tenure', 'monthly_charges',\n",
            "                                                   'total_charges'])])),\n",
            "                ('reduce_dim', 'passthrough'),\n",
            "                ('classifier',\n",
            "                 RandomForestClassifier(bootstrap=False,\n",
            "                                        class_weight='balanced',\n",
            "                                        criterion='entropy', max_depth=5,\n",
            "                                        n_estimators=200, n_jobs=-1))])\n",
            "INFO - model_repository.py:save_grid:57 - saving grid search results to ./MODELS/grid_RF.pkl\n"
          ]
        }
      ],
      "source": [
        "! python train.py --seed $SEED --testsplit $TEST_SPLIT_PCT --kfolds $KFOLDS $TRAIN_TEST_FILE_CLEAN $MODELS_DIR\n",
        "# ! python train.py --quick --seed $SEED --testsplit $TEST_SPLIT_PCT --kfolds $KFOLDS $TRAIN_TEST_FILE_CLEAN $MODELS_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbj5ihgaE-TP"
      },
      "source": [
        "## Criação de Ensemble com os melhores modelos treinados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3T-PET9B856",
        "outputId": "a4f8179d-7374-43c3-96ce-f7b4dc4d0d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO - ensembler.py:run:31 - starting ensembler\n",
            "INFO - ensembler.py:read_grids:23 - reading saved grids\n",
            "INFO - model_repository.py:list_grids:45 - listing saved grids on ./MODELS\n",
            "INFO - model_repository.py:load_grid:70 - loading grid search results from ./MODELS/grid_DT.pkl\n",
            "INFO - model_repository.py:load_grid:70 - loading grid search results from ./MODELS/grid_LOGREG.pkl\n",
            "INFO - model_repository.py:load_grid:70 - loading grid search results from ./MODELS/grid_GB.pkl\n",
            "INFO - model_repository.py:load_grid:70 - loading grid search results from ./MODELS/grid_NB.pkl\n",
            "INFO - model_repository.py:load_grid:70 - loading grid search results from ./MODELS/grid_ADA.pkl\n",
            "INFO - model_repository.py:load_grid:70 - loading grid search results from ./MODELS/grid_RF.pkl\n",
            "INFO - model_repository.py:load_grid:70 - loading grid search results from ./MODELS/grid_KNN.pkl\n",
            "INFO - data_loader.py:load_cleansed:62 - loading cleansed dataframe from ./DATA/telco-churn-train-test-clean.csv\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6338 entries, 0 to 6337\n",
            "Data columns (total 15 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   total_charges      6338 non-null   float64\n",
            " 1   monthly_charges    6338 non-null   float64\n",
            " 2   tenure             6338 non-null   int64  \n",
            " 3   monthly_contract   6338 non-null   int64  \n",
            " 4   two_year_contract  6338 non-null   int64  \n",
            " 5   fiber_optic        6338 non-null   int64  \n",
            " 6   electronic_check   6338 non-null   int64  \n",
            " 7   paperless_billing  6338 non-null   int64  \n",
            " 8   partner            6338 non-null   int64  \n",
            " 9   online_security    6338 non-null   int64  \n",
            " 10  tech_support       6338 non-null   int64  \n",
            " 11  online_backup      6338 non-null   int64  \n",
            " 12  dependents         6338 non-null   int64  \n",
            " 13  one_year_contract  6338 non-null   int64  \n",
            " 14  churn              6338 non-null   int64  \n",
            "dtypes: float64(2), int64(13)\n",
            "memory usage: 742.9 KB\n",
            "\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6338 entries, 0 to 6337\n",
            "Data columns (total 15 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   total_charges      6338 non-null   float64\n",
            " 1   monthly_charges    6338 non-null   float64\n",
            " 2   tenure             6338 non-null   int64  \n",
            " 3   monthly_contract   6338 non-null   int64  \n",
            " 4   two_year_contract  6338 non-null   int64  \n",
            " 5   fiber_optic        6338 non-null   int64  \n",
            " 6   electronic_check   6338 non-null   int64  \n",
            " 7   paperless_billing  6338 non-null   int64  \n",
            " 8   partner            6338 non-null   int64  \n",
            " 9   online_security    6338 non-null   int64  \n",
            " 10  tech_support       6338 non-null   int64  \n",
            " 11  online_backup      6338 non-null   int64  \n",
            " 12  dependents         6338 non-null   int64  \n",
            " 13  one_year_contract  6338 non-null   int64  \n",
            " 14  churn              6338 non-null   int64  \n",
            "dtypes: float64(2), int64(13)\n",
            "memory usage: 742.9 KB\n",
            "\n",
            "INFO - data_splitter.py:split:33 - splitting data set into train and test sets\n",
            "INFO - ensembler.py:compute_estimator_weights:45 - computing estimator weights\n",
            "INFO - ensembler.py:update_scores:38 - current top scores\n",
            "INFO - ensembler.py:update_scores:42 - \t1 - 0.6996350028703657 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:38 - current top scores\n",
            "INFO - ensembler.py:update_scores:42 - \t1 - 0.708331856807728 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t2 - 0.6996350028703657 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:38 - current top scores\n",
            "INFO - ensembler.py:update_scores:42 - \t1 - 0.7156672360149401 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t2 - 0.708331856807728 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t3 - 0.6996350028703657 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:38 - current top scores\n",
            "INFO - ensembler.py:update_scores:42 - \t1 - 0.7195702247390093 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t2 - 0.7156672360149401 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t3 - 0.708331856807728 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t4 - 0.6996350028703657 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:38 - current top scores\n",
            "INFO - ensembler.py:update_scores:42 - \t1 - 0.7582776387875008 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t2 - 0.7195702247390093 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t3 - 0.7156672360149401 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t4 - 0.708331856807728 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t5 - 0.6996350028703657 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:38 - current top scores\n",
            "INFO - ensembler.py:update_scores:42 - \t1 - 0.7624655378923719 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t2 - 0.7582776387875008 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t3 - 0.7195702247390093 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t4 - 0.7156672360149401 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t5 - 0.708331856807728 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t6 - 0.6996350028703657 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:38 - current top scores\n",
            "INFO - ensembler.py:update_scores:42 - \t1 - 0.7680652317200224 with 2 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t2 - 0.7624655378923719 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t3 - 0.7582776387875008 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t4 - 0.7195702247390093 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t5 - 0.7156672360149401 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t6 - 0.708331856807728 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t7 - 0.6996350028703657 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:38 - current top scores\n",
            "INFO - ensembler.py:update_scores:42 - \t1 - 0.7702793113957065 with 3 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t2 - 0.7680652317200224 with 2 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t3 - 0.7624655378923719 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t4 - 0.7582776387875008 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t5 - 0.7195702247390093 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t6 - 0.7156672360149401 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t7 - 0.708331856807728 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t8 - 0.6996350028703657 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:38 - current top scores\n",
            "INFO - ensembler.py:update_scores:42 - \t1 - 0.7709951310091638 with 4 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t2 - 0.7702793113957065 with 3 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t3 - 0.7680652317200224 with 2 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t4 - 0.7624655378923719 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t5 - 0.7582776387875008 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t6 - 0.7195702247390093 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t7 - 0.7156672360149401 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t8 - 0.708331856807728 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t9 - 0.6996350028703657 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:38 - current top scores\n",
            "INFO - ensembler.py:update_scores:42 - \t1 - 0.771951919601409 with 3 estimators and hard voting\n",
            "INFO - ensembler.py:update_scores:42 - \t2 - 0.7709951310091638 with 4 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t3 - 0.7702793113957065 with 3 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t4 - 0.7680652317200224 with 2 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t5 - 0.7624655378923719 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t6 - 0.7582776387875008 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t7 - 0.7195702247390093 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t8 - 0.7156672360149401 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t9 - 0.708331856807728 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:update_scores:42 - \t10 - 0.6996350028703657 with 1 estimators and soft voting\n",
            "INFO - ensembler.py:ensemble_models:91 - best combination of estimators: \n",
            "INFO - ensembler.py:ensemble_models:93 - \tPipeline(steps=[('feature_scaling',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  ['tenure', 'monthly_charges',\n",
            "                                                   'total_charges'])])),\n",
            "                ('reduce_dim', PCA(n_components=5)),\n",
            "                ('classifier', AdaBoostClassifier(n_estimators=100))])\n",
            "INFO - ensembler.py:ensemble_models:93 - \tPipeline(steps=[('feature_scaling',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('scaler',\n",
            "                                                                   StandardScaler())]),\n",
            "                                                  ['tenure', 'monthly_charges',\n",
            "                                                   'total_charges'])])),\n",
            "                ('reduce_dim', 'passthrough'),\n",
            "                ('classifier', GaussianNB(var_smoothing=0.1))])\n",
            "INFO - ensembler.py:ensemble_models:93 - \tPipeline(steps=[('feature_scaling',\n",
            "                 ColumnTransformer(remainder='passthrough',\n",
            "                                   transformers=[('num',\n",
            "                                                  Pipeline(steps=[('scaler',\n",
            "                                                                   'passthrough')]),\n",
            "                                                  ['tenure', 'monthly_charges',\n",
            "                                                   'total_charges'])])),\n",
            "                ('reduce_dim', 'passthrough'),\n",
            "                ('classifier',\n",
            "                 RandomForestClassifier(bootstrap=False,\n",
            "                                        class_weight='balanced',\n",
            "                                        criterion='entropy', max_depth=5,\n",
            "                                        n_estimators=200, n_jobs=-1))])\n",
            "INFO - ensembler.py:ensemble_models:94 - estimators weights: (5.5250829998632325, 5.5539550895877365, 5.591344869229035, 5.7042296613392605, 5.777006712660658, 5.832886573822399, 5.872449843148663)\n",
            "INFO - ensembler.py:ensemble_models:95 - Train Results\n",
            "INFO - model_evaluator.py:report_results:40 - accuracy score        : 0.7648782687105501\n",
            "INFO - model_evaluator.py:report_results:41 - precision score       : 0.538638985005767\n",
            "INFO - model_evaluator.py:report_results:42 - recall score          : 0.7935429056924383\n",
            "INFO - model_evaluator.py:report_results:43 - balanced acc. score   : 0.7740344169456361\n",
            "INFO - model_evaluator.py:report_results:44 - f1 score              : 0.6417038818275507\n",
            "INFO - model_evaluator.py:report_results:45 - confusion matrix\n",
            "INFO - model_evaluator.py:report_results:46 - \tTrue  Negative : 2459\n",
            "INFO - model_evaluator.py:report_results:47 - \tFalse Positive : 800\n",
            "INFO - model_evaluator.py:report_results:48 - \tFalse Negative : 243\n",
            "INFO - model_evaluator.py:report_results:49 - \tTrue  Positive : 934\n",
            "INFO - ensembler.py:ensemble_models:97 - Test Results\n",
            "INFO - model_evaluator.py:report_results:40 - accuracy score        : 0.7597266035751841\n",
            "INFO - model_evaluator.py:report_results:41 - precision score       : 0.5316622691292876\n",
            "INFO - model_evaluator.py:report_results:42 - recall score          : 0.7980198019801981\n",
            "INFO - model_evaluator.py:report_results:43 - balanced acc. score   : 0.771951919601409\n",
            "INFO - model_evaluator.py:report_results:44 - f1 score              : 0.6381631037212985\n",
            "INFO - model_evaluator.py:report_results:45 - confusion matrix\n",
            "INFO - model_evaluator.py:report_results:46 - \tTrue  Negative : 1042\n",
            "INFO - model_evaluator.py:report_results:47 - \tFalse Positive : 355\n",
            "INFO - model_evaluator.py:report_results:48 - \tFalse Negative : 102\n",
            "INFO - model_evaluator.py:report_results:49 - \tTrue  Positive : 403\n",
            "INFO - ensembler.py:ensemble_models:100 - refiting base classifiers on whole data set\n",
            "WARNING - ensembler.py:ensemble_models:106 - Whole data set results (has data leakage)\n",
            "INFO - model_evaluator.py:report_results:40 - accuracy score        : 0.7565478068791417\n",
            "INFO - model_evaluator.py:report_results:41 - precision score       : 0.5273514364423455\n",
            "INFO - model_evaluator.py:report_results:42 - recall score          : 0.7966706302021404\n",
            "INFO - model_evaluator.py:report_results:43 - balanced acc. score   : 0.7693619474034756\n",
            "INFO - model_evaluator.py:report_results:44 - f1 score              : 0.6346199384323941\n",
            "INFO - model_evaluator.py:report_results:45 - confusion matrix\n",
            "INFO - model_evaluator.py:report_results:46 - \tTrue  Negative : 3455\n",
            "INFO - model_evaluator.py:report_results:47 - \tFalse Positive : 1201\n",
            "INFO - model_evaluator.py:report_results:48 - \tFalse Negative : 342\n",
            "INFO - model_evaluator.py:report_results:49 - \tTrue  Positive : 1340\n",
            "INFO - model_repository.py:save_final_model:82 - saving final model to ./MODELS/final_model.pkl\n"
          ]
        }
      ],
      "source": [
        "! python ensembler.py --seed $SEED --testsplit $TEST_SPLIT_PCT --kfolds $KFOLDS $TRAIN_TEST_FILE_CLEAN $MODELS_DIR final_model.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QufWsjRjFieY"
      },
      "source": [
        "## Classificação do Arquivo de Hold Out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvEIA6jKCWh5",
        "outputId": "0232eb7b-1894-4e39-82f5-67a97bd1c291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO - classify.py:main:13 - starting classifier\n",
            "INFO - model_repository.py:load_final_model:88 - loading final model from ./MODELS/final_model.pkl\n",
            "INFO - data_loader.py:load_cleansed:62 - loading cleansed dataframe from ./DATA/telco-churn-holdout-clean.csv\n",
            "INFO - util.py:report_df:21 - <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 705 entries, 0 to 704\n",
            "Data columns (total 15 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   total_charges      705 non-null    float64\n",
            " 1   monthly_charges    705 non-null    float64\n",
            " 2   tenure             705 non-null    int64  \n",
            " 3   monthly_contract   705 non-null    int64  \n",
            " 4   two_year_contract  705 non-null    int64  \n",
            " 5   fiber_optic        705 non-null    int64  \n",
            " 6   electronic_check   705 non-null    int64  \n",
            " 7   paperless_billing  705 non-null    int64  \n",
            " 8   partner            705 non-null    int64  \n",
            " 9   online_security    705 non-null    int64  \n",
            " 10  tech_support       705 non-null    int64  \n",
            " 11  online_backup      705 non-null    int64  \n",
            " 12  dependents         705 non-null    int64  \n",
            " 13  one_year_contract  705 non-null    int64  \n",
            " 14  churn              705 non-null    int64  \n",
            "dtypes: float64(2), int64(13)\n",
            "memory usage: 82.7 KB\n",
            "\n",
            "INFO - model_evaluator.py:report_results:40 - accuracy score        : 0.7631205673758865\n",
            "INFO - model_evaluator.py:report_results:41 - precision score       : 0.5370370370370371\n",
            "INFO - model_evaluator.py:report_results:42 - recall score          : 0.7754010695187166\n",
            "INFO - model_evaluator.py:report_results:43 - balanced acc. score   : 0.7670441641029877\n",
            "INFO - model_evaluator.py:report_results:44 - f1 score              : 0.6345733041575492\n",
            "INFO - model_evaluator.py:report_results:45 - confusion matrix\n",
            "INFO - model_evaluator.py:report_results:46 - \tTrue  Negative : 393\n",
            "INFO - model_evaluator.py:report_results:47 - \tFalse Positive : 125\n",
            "INFO - model_evaluator.py:report_results:48 - \tFalse Negative : 42\n",
            "INFO - model_evaluator.py:report_results:49 - \tTrue  Positive : 145\n"
          ]
        }
      ],
      "source": [
        "! python classify.py ./MODELS/final_model.pkl $HOLD_OUT_FILE_CLEAN"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}